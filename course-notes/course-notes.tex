% !TeX spellcheck = en_US
\documentclass[12pt]{report}

\usepackage[a4paper,hmargin=3cm,vmargin=2.6cm]{geometry}
\usepackage{subfiles}
\usepackage[dvipsnames]{xcolor}
\usepackage{mathtools}
\usepackage{amssymb,dsfont,stmaryrd}
\usepackage[
	ruled,vlined,
	algochapter,
	linesnumbered
]{algorithm2e}
\usepackage[thmmarks,amsmath]{ntheorem}
\usepackage[
	ntheorem=true,framemethod=TikZ
]{mdframed}
\usepackage{hyperref,cleveref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{titlesec}

\usepackage[backend=biber,sorting=nyt]{biblatex}

\addbibresource{references.bib}

\hypersetup{
	colorlinks,
	urlcolor=NavyBlue,
	citecolor=ForestGreen
}

%%% Math macros %%%

\newcommand\RR{\mathbb{R}}
\newcommand\CC{\mathbb{C}}
\newcommand\ZZ{\mathbb{Z}}
\newcommand\NN{\mathbb{N}}
\newcommand\PP{\mathbb{P}}
\newcommand\TT{\mathbb{T}}
\DeclarePairedDelimiter{\intinterv}{\llbracket}{\rrbracket}
\newcommand{\scalemid}{\;\middle|\;}

\renewcommand{\epsilon}{\varepsilon}

\newcommand{\suchthat}{\mathrm{s.t.}}

\DeclareMathOperator*{\argmin}{\mathrm{argmin}}
\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\DeclareMathOperator{\sgn}{\mathrm{sgn}}
\DeclareMathOperator{\trace}{\mathrm{Tr}}
\DeclareMathOperator{\EE}{\mathbb{E}}
\newcommand{\der}{\operatorname{d\!}{}}

\newcommand{\calA}{\mathcal{A}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calL}{\mathcal{L}}

\newcommand{\TD}{\mathsf{TD}}


%%% Section titling setup %%%

\titleformat{\chapter}[display]
	{\normalfont\sffamily\huge\bfseries}
	{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titleformat*{\section}{\Large\bfseries\sffamily}
\titleformat*{\subsection}{\large\bfseries\sffamily}
\titleformat{\paragraph}[runin]{\sffamily\bfseries}{}{}{}[.]

%%% Colors %%%

\colorlet{lightblue}{RoyalBlue!20}
\colorlet{midblue}{RoyalBlue!72}
\colorlet{midgreen}{OliveGreen!65}
\colorlet{darkred}{Red!90!Black}

\newcommand{\redfont}{\color{darkred}}
\newcommand{\bluefont}{\color{Blue}}


%%% Theorem Environments %%%


\mdfsetup{
	linewidth=1pt
}

\newmdtheoremenv[
	linecolor=midgreen,
	backgroundcolor=OliveGreen!20
]{defn}{Definition}

\newmdtheoremenv[
	linecolor=black!40,
	backgroundcolor=black!7,
]{remark}{Remark}

\newmdtheoremenv[
	linecolor=BrickRed!80
]{example}{Example}


\newmdtheoremenv[
	linecolor=midblue,
	backgroundcolor=lightblue,
]{prop}{Proposition}

\newmdtheoremenv[
	linewidth=1pt,
	backgroundcolor=Aquamarine!40,
]{thm}{Theorem}





%%% Document title %%%

\title{
{
	\sffamily\Huge
	\textbf{Reinforcement Learning}\\
	{\bluefont-- Course notes --}
}\\[2ex]
\normalsize\sffamily\today
}
\date{}


\begin{document}

\maketitle


\chapter{Value functions, Dynamic programming}

\section{The value function}

The value function is a staple from the literature on dynamic programming, whether it be for discrete or continuous problems (as in control theory). It measures just how good a control $u$ -- or, in our case, a policy $\pi$ -- is regarding the desired target of our problem.


\begin{defn}[Value function]
The value function $V^\pi\colon \calS \to \RR$ of a policy $\pi$ is the expectation of the cumulative (discounted) future rewards starting from a point $s_0 = s$
\begin{equation}\label{eq:ValueFunctionDefn}
	V^\pi(s) \coloneqq
	\EE_{\tau\sim\pi}\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t)
	\scalemid s_0 = s
	\right]
\end{equation}
where the trajectory $\tau$ is generated under the policy $\pi$.

$T$ can be a random stopping time corresponding to the first time of arrival at a so-called \emph{terminal} state. It can also be $T = \infty$ for \emph{infinite horizon} problems.
\end{defn}


\begin{remark}[Notational abuse]
	Often, we will write the value functon at a state $s_t$, where it is implied we are at the $t$-th step in a trajectory, as
	\[
	V(s_t) = \EE_{\tau\sim\pi}\left[
	\sum_{{\redfont t'=t}}^T \gamma^{{\redfont t'-t}} r(s_{t'}, a_{t'})
	\scalemid s_t
	\right]
	\]
	This is coherent with the notion of cumulative (discounted) future rewards which defined the value function \eqref{eq:ValueFunctionDefn}, but not with the notation. In the finite horizon setting, a more correct notation would be to write $V(t, s_t)$ and make the dependence on the starting time $t$ of the discounting explicit.
\end{remark}


This notion can be generalized to the case where the rewards are generated by the transitions $(s_t, a_t, s_{t+1})$ rather than the (state, action) couple:
\[
	V^\pi(s) = \EE\left[
	\sum_{t=0}^{T-1} \gamma^t r(s_t, a_t, s_{t+1})
	\scalemid s_0 = s
	\right]
\]

Under a deterministic policy $\pi\colon \calS \to \calA$ and associated decision rule $d^\pi(s) = \pi(s)$, the dynamic programming principle leads to a dynamic programming equation called the \textbf{\bluefont Bellman equation}:
\begin{equation}\label{eq:ValueBellmanEqn}
	V^\pi(s) = r(s, \pi(s)) + \gamma\sum_{s'\in\calS} p(s, \pi(s), s')V^\pi(s')
\end{equation}
which is a fixed-point condition.

The Bellman equation can be used to evaluate a policy $\pi$ and compute its associated value function $V^\pi$.

Its fixed-point structure can be reformulated in terms of an operator on a function space, called the \textbf{Bellman operator}:
\[
	\calT^\pi v(s) \coloneqq
	r(s,\pi(s)) + \gamma\sum_{s'\in\calS}p(s,\pi(s),s') v(s')
\]



\begin{remark}
We have the following, possible generalizations (see Sutton's book \cite[][chap.\ 3,4]{Sutton1998} for further details):
\begin{itemize}
	\item for stochastic policies $\pi\colon \calS\times\calA \to \RR_+$, the sum in \cref{eq:ValueBellmanEqn} becomes 
	\[
	\sum_{s'\in\calS} \sum_{a\in\calA} \pi(s,a) p(s,a,s')V^\pi(s')
	\]
	\item in non-discrete state spaces, the sum can be replaced by an integral with respect to a measure $p(s,\pi(s),\der s')$
	\item if the rewards are given for transitions as $r(s,a,s')$, we introduce $r(s,a) = \sum_{s'\in\calS}r(s,a,s')$, and the Bellman equation can be rewritten
	\[
		V^\pi(s) = \sum_{s'\in\calS}
		r(s,\pi(s),s') + \gamma p(s,\pi(s),s')V^\pi(s')
	\]
\end{itemize}
\end{remark}



\subsection{The optimal value function}\label{sec:OptimalValueFunc}

Solving a Markov Decision Process involves finding an \textbf{\bluefont optimal} policy $\pi^*$ that will maximize the expected rewards in the long run when starting from a given state $s_0$ (or distribution $s_0 \sim p$).

\begin{defn}[Optimal policy and value function]
	Given a set of policies $\Pi$, the \emph{\bfseries\bluefont optimal value function} satisfies
	\begin{equation}
		V^*(s) = \max_{\pi} V^{\pi}(s)
	\end{equation}
	for every state $s$. An \emph{\bfseries\bluefont optimal policy} $\pi^*$ is one that satisfies the maximum.
\end{defn}

\begin{prop}[Optimal Bellman equation]
The optimal value function $V^*$ obeys a dynamic programming principle, the \emph{\bfseries\bluefont optimal Bellman equation}.
\begin{equation}\label{eq:ValueOptimalBellmanEq}
V^*(s) = \max_{a\in\calA}
\left\{
	r(s,a) + \sum_{s'\in\calS} p(s,a,s')V^*(s')
\right\}
\end{equation}
\end{prop}


This is also a fixed-point condition, which can once again be expressed in terms of an operator called the \textbf{Bellman optimal operator}:
\[
	\calT^*v(s) \coloneqq
	\max_{a\in\calA} \left\{
	r(s,a) + \gamma\sum_{s'\in\calS}p(s,a,s')v(s')
	\right\}.
\]
Then, we have that the optimal value function satisfies the equation
\[
	\calT^* V^* = V^*
\]


\subsection{The Value Iteration algorithm}

Under suitable hypotheses, it can be shown that the Bellman operators $\calT^\pi$ and $\calT^*$ are contractions with respect to the infinity norm with Lipschitz constant $\gamma$. This leads to the following algorithm:

\begin{algorithm}
\caption{Value iteration}
\KwIn{Rewards $r(s,a)$, transitions $p(s,a,s')$, initial value proposal $V_0$.}
\ForEach{$k=1,\ldots,K$}{
	$V_k \leftarrow \calT^* V_{k-1}$\;
}
\ForEach{$s\in\calS$}{
	$\pi_K(s) \leftarrow \argmax_{a\in\calA} \left\{r(s,a)+\gamma\sum_{s'\in\calS}p(s,a,s')V_K(s')\right\}$\;
}
\Return Policy $\pi_K$, value $V_K$\;
\end{algorithm}

This algorithm is especially useful for discrete state and action spaces. It can be significantly sped up if we have sparse representations of the rewards and transitions.


\section{The $\boldsymbol{Q}$-function}



\begin{defn}[State-action value function]
	The \textit{state-action value function} of a policy $\pi$ is the function $Q^\pi\colon \calS\times\calA\to \RR$ is defined by
	\begin{equation}
	Q^\pi(s, a) \coloneqq \EE_{\tau\sim \pi}
	\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t) \scalemid s_0 = s, a_0 = a
	\right]
	\end{equation}
	where the trajectory $\tau$ is generated under the policy $\pi$.
	The horizon $T$ of the problem can be finite or infinite ($T$ can be a stopping time).
\end{defn}



The state-action value function $Q^\pi$ has an obvious link to the value function $V^\pi$.
For any state $s\in\calS$, it holds that
\begin{equation}\label{eq:ValueFunctionExpectationLink}
	V^\pi(s) = \EE_{a\sim \pi(s,\cdot)}\left[
	Q^\pi(s, a)
	\right]
\end{equation}

\begin{defn}[Optimal action value function]
Optimal policies as defined in \cref{sec:OptimalValueFunc} also share the same \emph{\bfseries\bluefont optimal action value function}
\begin{equation}
	Q^*(s,a) = \max_\pi Q^\pi(s,a)
\end{equation}
for $s\in\calS$, $a\in\calA$.	
\end{defn}

We also have a link between the optimal value functions:
\[
	Q^*(s,a) = \EE_\pi\left[
	r_t + \gamma V^*(s_{t+1}) \scalemid
	s_t=s, a_t=a
	\right]
\]



\section{Temporal-difference estimation -- $\mathsf{TD}(0)$}

The real value function $V^\pi$ satisfies the Bellman equation. This means that the \textbf{\bluefont temporal difference error} of a good estimate $\widehat{V}^\pi$ of $V^\pi$, defined as
\[
	\delta_t = r_t + \gamma \widehat{V}^\pi(s_{t+1}) - \widehat{V}^\pi(s_t),
\]
should be small.


\subfile{parts/gradientmethods.tex}





\nocite{*}
\printbibliography


\end{document}

