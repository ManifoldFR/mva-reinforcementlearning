% !TeX spellcheck = en_US
\documentclass[12pt]{report}

\usepackage[a4paper,hmargin=3cm,vmargin=2.6cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{mathtools}
\usepackage{amssymb,dsfont,stmaryrd}
\usepackage[
	ruled,lined,
	algochapter,
	linesnumbered
]{algorithm2e}
\usepackage[thmmarks,amsmath]{ntheorem}
\usepackage[
	ntheorem=true,framemethod=TikZ
]{mdframed}
\usepackage{hyperref,cleveref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{titlesec}

\usepackage[backend=biber,sorting=nyt]{biblatex}

\addbibresource{references.bib}

\hypersetup{
	colorlinks,
	urlcolor=NavyBlue,
	citecolor=ForestGreen
}

%%% Math macros %%%

\newcommand\RR{\mathbb{R}}
\newcommand\CC{\mathbb{C}}
\newcommand\ZZ{\mathbb{Z}}
\newcommand\NN{\mathbb{N}}
\newcommand\PP{\mathbb{P}}
\newcommand\TT{\mathbb{T}}
\DeclarePairedDelimiter{\intinterv}{\llbracket}{\rrbracket}
\newcommand{\scalemid}{\;\middle|\;}

\renewcommand{\epsilon}{\varepsilon}

\newcommand{\suchthat}{\mathrm{s.t.}}

\DeclareMathOperator*{\argmin}{\mathrm{argmin}}
\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\DeclareMathOperator{\sgn}{\mathrm{sgn}}
\DeclareMathOperator{\trace}{\mathrm{Tr}}
\DeclareMathOperator{\EE}{\mathbb{E}}
\newcommand{\der}{\operatorname{d\!}{}}

\newcommand{\calA}{\mathcal{A}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calL}{\mathcal{L}}

\newcommand{\TD}{\mathsf{TD}}


%%% Section titling setup %%%

\titleformat{\chapter}[display]
	{\normalfont\sffamily\huge\bfseries}
	{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titleformat*{\section}{\Large\bfseries\sffamily}
\titleformat*{\subsection}{\large\bfseries\sffamily}
\titleformat{\paragraph}[runin]{\sffamily\bfseries}{}{}{}[.]

%%% Colors %%%

\colorlet{lightblue}{RoyalBlue!20}
\colorlet{midblue}{RoyalBlue!72}
\colorlet{midgreen}{OliveGreen!65}
\colorlet{darkred}{BrickRed}

\newcommand{\redfont}{\color{darkred}}
\newcommand{\bluefont}{\color{Blue}}


%%% Theorem Environments %%%


\mdfsetup{
	linewidth=1pt
}

\newmdtheoremenv[
	linecolor=midgreen,
	backgroundcolor=OliveGreen!20
]{defn}{Definition}

\newmdtheoremenv[
	linecolor=black!40,
	backgroundcolor=black!7,
]{remark}{Remark}

\newmdtheoremenv[
	linecolor=BrickRed!80
]{example}{Example}


\newmdtheoremenv[
	linecolor=midblue,
	backgroundcolor=lightblue,
]{prop}{Proposition}

\newmdtheoremenv[
	linewidth=1pt,
	backgroundcolor=Aquamarine!40,
]{thm}{Theorem}





%%% Document title %%%

\title{
{
	\sffamily\Huge
	\textbf{Reinforcement Learning}\\
	{\bluefont-- Course notes --}
}\\[2ex]
\normalsize\sffamily\today
}
\date{}


\begin{document}

\maketitle


\chapter{Value functions, Dynamic programming}

\section{The value function}

The value function is a staple from the literature on dynamic programming, whether it be for discrete or continuous problems (as in control theory). It measures just how good a control $u$ -- or, in our case, a policy $\pi$ -- is regarding the desired target of our problem.


\begin{defn}[Value function]
The value function $V^\pi\colon \calS \to \RR$ of a policy $\pi$ is the expectation of the cumulative (discounted) future rewards starting from a point $s_0 = s$
\begin{equation}\label{eq:ValueFunctionDefn}
	V^\pi(s) \coloneqq
	\EE_{\tau\sim\pi}\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t)
	\scalemid s_0 = s
	\right]
\end{equation}
where the trajectory $\tau$ is generated under the policy $\pi$.
\end{defn}

This notion can be generalized to the case where the rewards are generated by the transitions $(s_t, a_t, s_{t+1})$ rather than the (state, action) couple:
\[
	V^\pi(s) = \EE\left[
	\sum_{t=0}^{T-1} \gamma^t r(s_t, a_t, s_{t+1})
	\scalemid s_0 = s
	\right]
\]

Under a deterministic policy $\pi\colon \calS \to \calA$ and associated decision rule $d^\pi(s) = \pi(s)$, the dynamic programming principle leads to a dynamic programming equation called the \textbf{\bluefont Bellman equation}:
\begin{equation}\label{eq:ValueBellmanEqn}
	V^\pi(s) = r(s, \pi(s)) + \gamma\sum_{s'\in\calS} p(s, \pi(s), s')V^\pi(s')
\end{equation}
which is a fixed-point condition.

The Bellman equation can be used to evaluate a policy $\pi$ and compute its associated value function $V^\pi$.

Its fixed-point structure can be reformulated in terms of an operator on a function space, called the \textbf{Bellman operator}:
\[
	\calT^\pi v(s) \coloneqq
	r(s,\pi(s)) + \gamma\sum_{s'\in\calS}p(s,\pi(s),s') v(s')
\]



\begin{remark}
We have the following, possible generalizations (see Sutton's book \cite[][chap.\ 3,4]{Sutton1998} for further details):
\begin{itemize}
	\item for stochastic policies $\pi\colon \calS\times\calA \to \RR_+$, the sum in \cref{eq:ValueBellmanEqn} becomes 
	\[
	\sum_{s'\in\calS} \sum_{a\in\calA} \pi(s,a) p(s,a,s')V^\pi(s')
	\]
	\item in non-discrete state spaces, the sum can be replaced by an integral with respect to a measure $p(s,\pi(s),\der s')$
	\item if the rewards are given for transitions as $r(s,a,s')$, we introduce $r(s,a) = \sum_{s'\in\calS}r(s,a,s')$, and the Bellman equation can be rewritten
	\[
		V^\pi(s) = \sum_{s'\in\calS}
		r(s,\pi(s),s') + \gamma p(s,\pi(s),s')V^\pi(s')
	\]
\end{itemize}
\end{remark}


\begin{remark}[Notational abuse]
	Often, we will write the value functon at a state $s_t$, where it is implied we are at the $t$-th step in a trajectory, as
	\[
		V(s_t) = \EE_{\tau\sim\pi}\left[
		\sum_{{\redfont t'=t}}^T \gamma^{{\redfont t'-t}} r(s_{t'}, a_{t'})
		\scalemid s_t
		\right]
	\]
	This is coherent with the notion of cumulative (discounted) future rewards which defined the value function \eqref{eq:ValueFunctionDefn}, but not with the notation; a more correct notation would be to write $V(t, s_t)$ and make the dependence on the starting time $t$ of the discounting explicit.
\end{remark}

\subsection{The optimal value function}

Solving a Markov Decision Process involves finding the \textbf{\bluefont optimal} policy $\pi^*$ that will maximize the expected rewards when starting at a point $s_0$.

\begin{defn}[Optimal policy and value function]
	Given a set of policies $\Pi$, an optimal policy $\pi^*$.
	
	The associated optimal value function is
	\[
		V^* \coloneqq V^{\pi^*}
	\]
\end{defn}

\begin{prop}[Optimal Bellman equation]
The optimal value function $V^*$ obeys a dynamic programming principle, the {\normalfont\bfseries\bluefont optimal Bellman equation}.
\begin{equation}\label{eq:ValueOptimalBellmanEq}
V^*(s) = \max_{a\in\calA}
\left\{
	r(s,a) + \sum_{s'\in\calS} p(s,a,s')V^*(s')
\right\}
\end{equation}
\end{prop}


This is also a fixed-point condition, which can once again be expressed in terms of an operator called the \textbf{Bellman optimal operator}:
\[
	\calT^*v(s) \coloneqq
	\max_{a\in\calA} \left\{
	r(s,a) + \gamma\sum_{s'\in\calS}p(s,a,s')v(s')
	\right\}
\]


\subsection{The Value Iteration algorithm}

Under suitable hypotheses, it can be shown that the Bellman operators $\calT^\pi$ and $\calT^*$ are contractions with respect to the infinity norm with Lipschitz constant $\gamma$. This leads to the following algorithm:

\begin{algorithm}
\caption{Value iteration}
\KwIn{Rewards $r(s,a)$, transition probabilities $p(s,a,s')$, initial value proposal $V_0$.}
\ForEach{$k=1,\ldots,K$}{
	$V_k \leftarrow \calT^* V_{k-1}$\;
}
\ForEach{$s\in\calS$}{
	$\pi_K(s) \leftarrow \argmax_{a\in\calA} \left\{r(s,a)+\gamma\sum_{s'\in\calS}p(s,a,s')V_K(s')\right\}$\;
}
\Return Policy $\pi_K$, value $V_K$\;
\end{algorithm}

This algorithm is especially useful for discrete state and action spaces. It can be significantly sped up if we have sparse representations of the rewards and transitions.


\section{The $\boldsymbol{Q}$-function}



\begin{defn}[State-action value function]
	The \textit{state-action value function} of a policy $\pi$ is the function $Q^\pi\colon \calS\times\calA\to \RR$ is defined by
	\begin{equation}
	Q^\pi(s, a) \coloneqq \EE_{\tau\sim \pi}
	\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t) \scalemid s_0 = s, a_0 = a
	\right]
	\end{equation}
	where the trajectory $\tau$ is generated under the policy $\pi$.
	The horizon $T$ of the problem can be finite or infinite ($T$ can be a stopping time).
\end{defn}



The state-action value function $Q^\pi$ has an obvious link to the value function $V^\pi$.

\begin{prop}\label{prop:ValueFunctionExpectationLink}
	For any state $s\in\calS$, it holds that
	\begin{equation}
	V^\pi(s) = \EE_{a\sim \pi(s,\cdot)}\left[
	Q^\pi(s, a)
	\right]
	\end{equation}
\end{prop}



\section{Temporal-difference estimation -- $\mathsf{TD}(0)$}

The real value function $V^\pi$ satisfies the Bellman equation. This means that the \textbf{\bluefont temporal difference error} of a good estimate $\widehat{V}^\pi$ of $V^\pi$, defined as
\[
	\delta_t = r_t + \gamma \widehat{V}^\pi(s_{t+1}) - \widehat{V}^\pi(s_t),
\]
should be small.






\chapter{Approximate solving of Markov Decision Processes}


Solving MDPs is seeking the maximizing policy of the value function. For approximate solving of MDPs, we target what could be a more general \textbf{\bluefont policy performance metric}. Often, it is indeed connected to the value function
\begin{equation}\label{eq:PolicyPerfCumReward}
J(\pi) = \EE_{\tau\sim\pi}\left[
	\sum_{t=0}^{T} \gamma^t r_t
\right] =
	\EE_{\tau\sim\pi}
	\left[R(\tau)\right]
\end{equation}
where $\tau = \left\{ s_1,a_1,r_1,\ldots,s_{T-1},a_{T-1},r_{T-1},s_{T}\right\}$ and $R(\tau) = \sum_{t=0}^T \gamma^t r_t$ is the total return of the trajectory $\tau$.
We use the shorthand $r_t$ for the reward $r(s_t, a_t)$ -- or more generally a transition reward $r(s_t, a_t, s_{t+1})$ or stochastic reward distributed as $r_t \sim p(\cdot, s_t, a_t, s_{t+1})$. The expectation $J$ is either conditional on a given starting point $s_0$, or on a distribution for it.\footnote{For instance, OpenAI Gym's \texttt{CartPole-v1} environment has a stochastic initial state $s_0$.}

We seek to compute the maximizing policy in a parametric search space $\{\pi_\theta: \theta\in\Theta\}$:
\[
	\max_\theta J(\pi_\theta)
\]

Iterative methods could be used if $J$ could be computed in closed form with given reward and transition structures; if these are not given they could be estimated by Monte Carlo methods, which would be very expensive and wasteful.

Instead, we will update the estimate as we go and simulate trajectories, by iteratively updating the policy parameter $\theta$ using a gradient ascent method with an estimated gradient.

\begin{prop}[Gradient under a parametric law]\label{prop:parametricGradient}
	Given a set of probability models $\{P_\theta: \theta\in\Theta\subseteq \RR^d\}$ on a set $\calX$ and a function $f\colon \calX\to \RR$, we have that
	\[
	\nabla_\theta \EE_{X\sim P_\theta}[f(X)]
	= \EE_{X\sim P_\theta}[f(X)\nabla_\theta \log P_\theta(X)]
	\]
	This is a useful property for deriving estimators of the derivatives in optimization problems with stochastic objectives.
	
	Generalization to the case where $f$ also depends on $\theta$ is straightforward.
\end{prop}

This can be shown either by either writing the expectation as an integral, or by a change of measures with a Radon-Nikodym derivative.

\Cref{prop:parametricGradient} allows us to write the gradient of \eqref{eq:PolicyPerfCumReward}, called the \textbf{\bluefont policy gradient} as an expectation:
\begin{equation}\label{eq:PolicyGradient}
	\nabla_\theta J(\pi_\theta) =
	\EE_{\tau\sim\pi_\theta}\left[
	R(\tau)\sum_{t=0}^T\nabla_\theta \log\pi_\theta(s_t, a_t)
	\right]
\end{equation}
and we will need to derive estimations for this quantity.

There are other ways of writing the policy gradient, such as (see \cite[][chap.\ 13]{Sutton1998})
\begin{align*}
	\nabla_\theta J(\pi_\theta)
	&=
	\EE_s\left[
	\sum_a Q^{\pi_\theta}(s,a) \nabla\pi_\theta(s,a)
	\right]
\end{align*}




\section{Monte Carlo policy gradient: the REINFORCE algorithm}

\paragraph{The idea} The policy gradient \eqref{eq:PolicyGradient} is an expectation which can be estimated using Monte Carlo approximation. We obtain the following estimate:
\begin{equation}\label{eq:MCPolicyGrad}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M R(\tau_i)
	\sum_{t=0}^{T_i}
	\nabla_\theta
	\log\pi_\theta(s_t^i, a_t^i)
\end{equation}
This is an unbiased Monte Carlo estimate of the policy gradient. It only requires suitable regularity of the parametric policy model $\theta \longmapsto \pi_\theta$.


\begin{remark}\label{rmk:PolicyGradPseudoLoss} \Cref{eq:MCPolicyGrad} can be used as-is for functions with simple closed-form derivatives.
In an automatic differentiation framework such as {\normalfont\sffamily PyTorch}, we can instead get the policy gradient from a computational graph with the following pseudo-loss function:
\begin{equation}\label{eq:PolicyGradPseudoLoss}
\begin{aligned}
	\tilde{J}(\theta)
	&= \frac{1}{M}\sum_{i=1}^M R(\tau_i)
	\sum_{t=0}^{T_i} \log\pi_\theta(s_t^i, a_t^i)\\
	&=
	\frac{1}{M}\sum_{i=1}^M
	\left(
		\sum_{t=0}^{T_i} \gamma^t r^i_{t}
	\right)
	\sum_{t=0}^{T_i} \log\pi_\theta(s_t^i, a_t^i)
\end{aligned}
\end{equation}
\end{remark}

We will simulate multiple trajectories (\textit{episodes}) to perform the gradient update: this is the similar to what is done with batch, mini-batch or stochastic gradient steps.

\begin{algorithm}
\caption{Monte Carlo Policy Gradient (REINFORCE)}\label{algo:REINFORCEbasic}
\KwIn{Arbitrary initial policy $\pi_{\theta_0}$.}
\KwOut{Optimal parametric policy $\pi_{\theta^*}$.}
\Repeat{finished}{
	Simulate a trajectory $\tau$\;
	$\boldsymbol{g} \leftarrow (\sum_{t=0}^T \gamma^t r_t)\sum_{t=0}^T \nabla_\theta \log\pi_\theta(s_t, a_t)$\tcp*{policy gradient}
	$\theta \leftarrow \theta + \alpha\boldsymbol{g}$\;
}
\end{algorithm}

This algorithm can be modified in several ways, by performing the gradient step at every time in the process, or on the entire batch of trajectories.

\subsection{Variance reduction: temporal structure and baselines}

\paragraph{Temporal structure} We can re-weigh the log-probability gradients in \cref{eq:PolicyGradient} by exploiting the fact that, for any time $t$, the cumulative rewards $\sum_{t'=0}^{t-1} \gamma^{t'} r_{t'}$ from $0$ to $t-1$ are measurable with respect to the trajectory up to $t$, $\tau_{0:t}$:
\begin{prop}
The policy gradient \eqref{eq:PolicyGradient} can be rewritten as
\begin{equation}
\nabla_\theta J(\pi_\theta) =
\EE\left[
	\sum_{t=0}^T
	{\redfont
		\sum_{t'=t}^T \gamma^{t'} r_{t'}
	}
	\nabla_\theta \log\pi_\theta(s_t,a_t)
\right]
\end{equation}
which leads to the policy gradient estimate
\begin{equation}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^{T_i}
	{\redfont \gamma^t G^i_t}
	\nabla_\theta \log\pi_\theta (s_t^i, a_t^i)
\end{equation}
where\footnote{This quantity is an estimate of the $Q$-function $Q^\pi(s_t, a_t) = \EE [\sum_{t'=t}^T \gamma^{t'-t} r_{t'} \,|\, s_{t}, a_{t} ]$.} $G^i_t = \sum_{t'=t}^T \gamma^{t'-t}r_{t'}$.
\end{prop}


The following algorithm provides an efficient recursive method for computing the returns $G^i_t$:

\begin{algorithm}
	\caption{Computing the returns}\label{algo:ComputeReturnsEfficient}
	\KwIn{The rewards $(r_t)_{0\leq t\leq T}$, discount factor $\gamma$}
	\KwOut{The array of discounted returns.}
	$R_T \leftarrow r_T$\;
	\ForEach{$t\leftarrow T-1$ \KwTo $0$}{
		$G_t \leftarrow r_t + \gamma G_{t+1}$\;
	}
	\Return $(G_t)_{0\leq t\leq T}$\;
\end{algorithm}


\paragraph{Baselines} Given any \textbf{\bluefont baseline} function $b\colon \calS \to \RR$, we can rewrite the policy gradient again as
\begin{equation}
	\nabla_\theta J(\pi_\theta) =
	\EE_\pi \left[
	\sum_{t=0}^T
	\left(
		\sum_{t'=t}^T \gamma^{t'} r_{t'} - {\redfont b(s_t)}
	\right)
	\nabla_\theta \log\pi_\theta(s_t, a_t)
	\right]
\end{equation}
The resulting policy gradient estimate we get is
\begin{equation}\label{eq:MCPolicyGradBaseline}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^{T_i}
	\gamma^t (G^i_t - {\redfont b(s^i_t)})
	\nabla_\theta \log\pi_\theta (s_t^i, a_t^i)
\end{equation}
which is an unbiased estimate.

It can be shown that the ``best" baseline in terms of variance reduction $b^*$ is the value function:
\[
	b^*(s_t) = \EE_{\pi_\theta}\left[\sum_{t'=t}^T \gamma^{t'-t} r_{t'} \scalemid s_t \right]
\]
\ldots which we are already trying to approximate. This suggests that we use some kind of \textbf{bootstrap} estimate for the baseline.

An obvious, nonparametric baseline is Monte Carlo estimation:
\[
	\widehat{b}_t =
	\frac{1}{M}\sum_{i=1}^M G^i_t
\]
This baseline is an unbiased estimate: following \cref{prop:ValueFunctionExpectationLink} the expectation of the $Q$-function estimate $G_t$ under the policy $\pi_\theta$ is
\[
	\EE_{a_t\sim \pi_\theta(s_t,\cdot)}
	\EE_{\pi_\theta}
	\left[G_t\right] =
	\EE_{a_t\sim\pi_\theta(s_t,\cdot)}
	\EE_{\pi_\theta}
	\left[
	\sum_{t'=t}^T \gamma^{t'-t}r_{t'}
	\scalemid s_t,a_t
	\right] = V^\pi(s_t)
\]


\subsection{Parametric Bootstrapping of the baseline}

We define the bootstrap estimate $b(s) \coloneqq \widehat{v}_\nu(s)$ lying in a parametric search space $\{\widehat{v}_\nu : \nu \in \mathcal{V}\}$. The value parameter can be iteratively updated using gradient steps alternatively with the policy parameter $\theta$.

For a given trajectory sample $\tau=\{s_0,a_0,r_0,\ldots\}$, introduce the mean-squared error between the forward cumulative rewards (a nonparametric estimate of the value function) and the output of the value model:
\[
	\calL(\nu; \tau) =
	\sum_{t=0}^{T} \left(G_t
	- \widehat{v}_\nu(s_t)
	\right)^2
\]
Then before each update of the policy $\pi_\theta$, update the value parameter $\nu$ using either the gradient of $\calL$.

The adapted episodic learning algorithm with an adaptive baseline is as follows:
\begin{algorithm}
\caption{REINFORCE with parametric baseline}\label{algo:REINFORCEparamBaseline}
\Repeat{finished}{
	Simulate a trajectory $\tau$\;
	Compute the returns $(G_t)$ of the trajectory\;
	$\boldsymbol{g} \leftarrow \sum_{t=0}^T \gamma^t (G_t - \widehat{v}_\nu(s_t))\nabla_\theta \log\pi_\theta (s_t,a_t)$ \tcp*{policy gradient}
	$\nu \leftarrow \nu - \beta\nabla_\nu \calL(\nu; \tau)$ \tcp*{update value estimate}
	$\theta \leftarrow \theta + \alpha \boldsymbol{g}$ \tcp*{update policy}
}
\end{algorithm}

As before (see \cref{rmk:PolicyGradPseudoLoss}), this algorithm can be implemented within an automatic differentiation framework such as \textsf{PyTorch} by defining the right computational graphs. The associated pseudo-loss would be
\[
	\tilde{J}(\theta) = 
	\sum_{t=0}^T \gamma^t (G_t - \widehat{v}_\nu(s_t)) \log\pi_\theta(s_t, a_t)
\]

\begin{remark}
The difference between the returns and the value approximation $G_t - \widehat{v}_\nu(s_t)$ is a (biased) estimate of the temporal-difference error $\delta_t = r_t + \gamma \widehat{v}(s_{t+1}) - \widehat{v}(s_t)$. Thus, the mean-squared error $\calL$ can be seen as an estimate of the Bellman error of the value proposal $\widehat{v}_\nu$.
\end{remark}



\section{Parametric approximation: Actor-Critic algorithms}


\paragraph{The idea} The enhanced REINFORCE algorithm builds estimates of the $Q$-function to compute the policy gradient as it runs: this is computationally expensive and may lead to high variance. To combat this, it might be a good idea to \textit{learn} from the $Q$-function estimates in a way that gives a consistent estimate that follows the policy gradient updates.

To achieve this, the class of \textbf{\bluefont actor-critic methods} introduces a second search space for approximation of the state(-action) value function.


\subsection{Actor-Critic}


The policy learning is still done by gradient ascent following a policy gradient estimate of the form \cref{eq:MCPolicyGradBaseline} -- but this time, we replace the Monte Carlo estimate $G^i_t$ of the $Q$-function by a parametric estimator $\widehat{q}_\omega(s_t, a_t)$
\begin{equation}\label{eq:ActorPolicyGrad}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^{T_i} \widehat{q}_\omega(s^i_t,a^i_t)
	\nabla_\theta \log\pi_\theta(s^i_t, a^i_t)
\end{equation}

\begin{algorithm}
\caption{Actor-Critic}\label{algo:BaseActorCritic}
\KwIn{Initial policy parameter $\theta$, value parameter $\omega$}
\KwOut{Policy $\pi_{\theta^*}$, action-state value approximation $\widehat{q}_{\omega^*}$}
\Repeat{finished}{
	Simulate trajectory $\tau$\;
	\ForEach{$t=0,\ldots,T$}{
		$\delta_t \leftarrow r_t + \gamma \widehat{q}_\omega(s_{t+1},a_{t+1}) - \widehat{q}_\omega(s_t,a_t)$ \tcp*{$\TD(0)$ error}
	}
	$\omega \leftarrow \omega + \beta \sum_{t=0}^T\delta_t \nabla_\omega \widehat{q}_\omega (s_t,a_t)$\;
	$\theta \leftarrow \theta + \alpha \sum_{t=0}^T\widehat{q}_\omega(s_t,a_t) \nabla_\theta \log\pi_\theta(s_t, a_t)$\;
}
\end{algorithm}

\begin{remark}
To leverage automatic differentiation, appropriate pseudo-losses to define a computational graph for the updates in \cref{algo:BaseActorCritic} are
\begin{subequations}
\begin{align}
	C(\omega) &= \frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^T\delta^i_t  \widehat{q}_\omega(s^i_t,a^i_t)  \\
	\tilde{J}(\theta) &=
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^T \widehat{q}_\omega(s^i_t,a^i_t) \log\pi_\theta(s^i_t,a^i_t)
\end{align}
\end{subequations}
where the advantage estimations $\delta_t$ must be detached from the graph.
\end{remark}


The value update step in \cref{algo:BaseActorCritic} can also be re-cast as a supervised regression step, where we fit the (bootstrapped) returns $G_{t:t+1} = r_t + \widehat{q}_\omega(s_{t+1},a_{t+1})$ to the value model predictions $\widehat{q}_\omega(s_t,a_t)$, with a mean-squared loss
\[
	\calL(\omega; \tau) =
	\sum_{t=0}^T (G_{t:t+1} - \widehat{q}_\omega(s_t,a_t))^2
\]
We then perform a semi-gradient update step, taking the gradient with respect to $\omega$ whilst ignoring the dependency of the return estimate $G_{t:t+1}$ on it (formally, we would be \textit{detaching} $G_{t:t+1}$ from the computational graph). This is called \textbf{one-step actor-critic}. It does not have high sample efficiency because the return estimates ignore the (actual) future rewards and we often end up bootstrapping too much.


\subsection{Actor-critic with baselines: Advantage Actor-Critic (A2C)}

As we've seen before, a good baseline to introduce in the policy gradient \eqref{eq:PolicyGradient} is the value function.

The coefficient before the log-probability gradient becomes an estimate of $Q^\pi(s,a) - V^\pi(s) = \EE_{s'}[r(s,a) + \gamma V^\pi(s')\mid s,a]$. We introduce the \textbf{advantage} function
\[
	A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s).
\]



\nocite{*}
\printbibliography


\end{document}

