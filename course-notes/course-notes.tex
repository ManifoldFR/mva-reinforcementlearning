% !TeX spellcheck = en_US
\documentclass[12pt]{report}

\usepackage[a4paper,hmargin=3cm,vmargin=2.6cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{mathtools}
\usepackage{amssymb,dsfont,stmaryrd}
\usepackage[thmmarks,amsmath]{ntheorem}
\usepackage[
	ntheorem=true,framemethod=tikz
	]{mdframed}
\usepackage{algorithm2e}
\usepackage{hyperref,cleveref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{titlesec}

\usepackage[backend=biber,sorting=nyt]{biblatex}

\addbibresource{references.bib}

\hypersetup{
	colorlinks,
	urlcolor=NavyBlue,
	citecolor=ForestGreen
}

%%% Math macros %%%

\newcommand\RR{\mathbb{R}}
\newcommand\CC{\mathbb{C}}
\newcommand\ZZ{\mathbb{Z}}
\newcommand\NN{\mathbb{N}}
\newcommand\PP{\mathbb{P}}
\newcommand\TT{\mathbb{T}}
\DeclarePairedDelimiter{\intinterv}{\llbracket}{\rrbracket}
\newcommand{\scalemid}{\;\middle|\;}

\renewcommand{\epsilon}{\varepsilon}

\newcommand{\suchthat}{\mathrm{s.t.}}

\DeclareMathOperator*{\argmin}{\mathrm{argmin}}
\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\DeclareMathOperator{\sgn}{\mathrm{sgn}}
\DeclareMathOperator{\trace}{\mathrm{Tr}}
\DeclareMathOperator{\EE}{\mathbb{E}}
\newcommand{\der}{\operatorname{d\!}{}}

\newcommand{\calA}{\mathcal{A}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calL}{\mathcal{L}}

\newcommand{\TD}{\mathsf{TD}}


%%% Section titling setup %%%

\titleformat{\chapter}[display]
	{\normalfont\sffamily\huge\bfseries}
	{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titleformat*{\section}{\Large\bfseries\sffamily}
\titleformat*{\subsection}{\large\bfseries\sffamily}
\titleformat{\paragraph}[runin]{\sffamily\bfseries}{}{}{}[.]

%%% Colors %%%

\colorlet{lightblue}{RoyalBlue!20}
\colorlet{midblue}{RoyalBlue!72}
\colorlet{midgreen}{OliveGreen!65}
\colorlet{darkred}{Red!75!black}

\newcommand{\redfont}{\color{darkred}}
\newcommand{\bluefont}{\color{Blue}}


%%% Theorem Environments %%%


\mdfsetup{
	linewidth=1pt
}

\newmdtheoremenv[
	linecolor=midgreen,
	backgroundcolor=OliveGreen!20
]{defn}{Definition}

\newmdtheoremenv[
	linecolor=black!40,
	backgroundcolor=black!7,
]{remark}{Remark}

\newmdtheoremenv[
	linecolor=BrickRed!80
]{example}{Example}


\newmdtheoremenv[
	linecolor=midblue,
	backgroundcolor=lightblue,
]{prop}{Proposition}

\newmdtheoremenv[
	linewidth=1pt,
	backgroundcolor=Aquamarine!40,
]{thm}{Theorem}





%%% Document title %%%

\title{
{\sffamily\Huge
	\textbf{Reinforcement Learning}\\
	{\bluefont-- Course notes --}
}
}
\date{}


\begin{document}

\maketitle


\chapter{Dynamic programming}

\section{The value function}

The value function is a staple from the literature on dynamic programming, whether it be for discrete or continuous problems (as in control theory). It measures just how good a control $u$ -- or, in our case, a policy $\pi$ -- is regarding the desired target of our problem.


\begin{defn}[Value function]
The value function $V^\pi\colon \calS \to \RR$ of a policy $\pi$ is the expectation of the cumulative (discounted) future rewards starting from a point $s_0 = s$
\begin{equation}
	V^\pi(s) \coloneqq
	\EE_{\tau\sim\pi}\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t)
	\scalemid s_0 = s
	\right]
\end{equation}
where the trajectory $\tau$ is generated under the policy $\pi$.
\end{defn}

This notion can be generalized to the case where the rewards are generated by the transitions $(s_t, a_t, s_{t+1})$ rather than the (state, action) couple:
\[
	V^\pi(s) = \EE\left[
	\sum_{t=0}^{T-1} \gamma^t r(s_t, a_t, s_{t+1})
	\scalemid s_0 = s
	\right]
\]

Under a deterministic policy $\pi\colon \calS \to \calA$ and associated decision rule $d^\pi(s) = \pi(s)$, the dynamic programming principle leads to a dynamic programming equation called the \textbf{\bluefont Bellman equation}:
\begin{equation}
	V^\pi(s) = r(s, \pi(s)) + \gamma\sum_{s'\in\calS} p(s, \pi(s), s')V^\pi(s')
\end{equation}
With generalizations:
\begin{itemize}
	\item for stochastic policies $\pi\colon \calS\times\calA \to \RR_+$, the sum becomes $\sum_{s'\in\calS} \sum_{a\in\calA} \pi(s,a) p(s,a,s')V^\pi(s')$
	\item non-discrete state space, the sum can be replaced by an integral with respect to a measure $p(s,\pi(s),\der s')$ -- see Sutton's book \cite{Sutton1998}
	\item for a transition reward $r(s,a,s')$, we introduce $r(s,a) = \sum_{s'\in\calS}r(s,a,s')$.
\end{itemize}


\section{The $\boldsymbol{Q}$-function}



\begin{defn}[State-action value function]
	The \textit{state-action value function} of a policy $\pi$ is the function $Q^\pi\colon \calS\times\calA\to \RR$ is defined by
	\begin{equation}
	Q^\pi(s, a) \coloneqq \EE_{\tau\sim \pi}
	\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t) \scalemid s_0 = s, a_0 = a
	\right]
	\end{equation}
	where the trajectory $\tau$ is generated under the decision rule $d^\pi$.
	The horizon $T$ of the problem can be finite or infinite ($T$ can be a stopping time).
\end{defn}


\section{Temporal-difference estimation -- $\mathsf{TD}(0)$}

The real value function $V^\pi$ satisfies the Bellman equation. This means that the \textbf{\bluefont temporal difference error} of a good estimate $\hat{v}^\pi$ of $V^\pi$, defined as
\[
	\delta_t = r_t + \gamma \hat{v}^\pi(s_{t+1}) - \hat{v}^\pi(s_t),
\]
should be small.






\chapter{Approximate solving of Markov Decision Processes}


Solving MDPs is seeking the maximizing policy of the value function. For approximate solving of MDPs, we target what could be a more general \textbf{\bluefont policy performance metric}. Often, it is connected to the value function: the expected (discounted) cumulative reward of the policyÂ¨
\begin{equation}\label{eq:PolicyPerfCumReward}
J(\pi) = \EE_{\tau\sim\pi}\left[
	\sum_{t=0}^{T} \gamma^t r_t
\right] =
	\EE_{\tau\sim\pi}
	\left[R(\tau)\right]
\end{equation}
where $\tau = \left\{ s_1,a_1,r_1,\ldots,s_{T-1},a_{T-1},r_{T-1},s_{T}\right\}$ and $R(\tau) = \sum_{t=0}^T \gamma^t r_t$.

We seek to compute the maximizing policy in a parametric search space $\{\pi_\theta: \theta\in\Theta\}$:
\[
	\max_\theta J(\pi_\theta)
\]

The expectation $J$ could be computed if we are given the complete structure of the Markov decision process: the transition probabilities $p(s,a,s')$ and reward function $r(s,a)$. But then we could just use the usual $Q$-learning algorithm.

Instead, we can use a gradient ascent method, by iteratively updating the policy parameter $\theta$ using a direction provided by the gradient.

\begin{prop}[Gradient under a parametric law]\label{prop:parametricGradient}
	Given a set of probability models $\{P_\theta: \theta\in\Theta\subseteq \RR^d\}$ on a set $\calX$ and a function $f\colon \calX\to \RR$, we have that
	\[
	\nabla_\theta \EE_{X\sim P_\theta}[f(X)]
	= \EE_{X\sim P_\theta}[f(X)\nabla_\theta \log P_\theta(X)]
	\]
	This is a useful property for deriving estimators of the derivatives in optimization problems with stochastic objectives.
\end{prop}

This can be shown either by either writing the expectation as an integral, or by a change of measures with a Radon-Nikodym derivative.

\Cref{prop:parametricGradient} allows us to write the gradient of \eqref{eq:PolicyPerfCumReward}, called the \textbf{\bluefont policy gradient} as
\begin{equation}\label{eq:PolicyGradient}
	\nabla_\theta J(\pi_\theta) =
	\EE_{\tau\sim\pi_\theta}\left[
	R(\tau)\sum_{t=0}^T\nabla_\theta \log\pi_\theta(s_t, a_t)
	\right]
\end{equation}
and we will need to derive estimations for this quantity.

This makes sense in the finite (or almost surely finite) horizon.


\section{Monte Carlo policy gradient: the REINFORCE algorithm}

\paragraph{The idea} The policy performance $J$ using a Monte Carlo approximation:
\begin{equation}\label{eq:MCCumReward}
	\widehat{J}(\pi_\theta) = \frac{1}{M}
	\sum_{i=1}^{M}\sum_{t=0}^{T_i} \gamma^t r^i_t =
	\frac{1}{M}\sum_{i=1}^M R(\tau_i)
\end{equation}
where $\tau_i$ are simulated trajectories under the policy $\pi_\theta$, and \cref{eq:PolicyGradient} 

We obtain the following estimate:
\begin{equation}\label{eq:MCPolicyGrad}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M R(\tau_i)
	\sum_{t=0}^{T_i}
	\nabla_\theta
	\log\pi_\theta(s_t^i, a_t^i)
\end{equation}
This is an unbiased Monte Carlo estimate of the policy gradient. It only requires suitable regularity of the parametric policy model $\theta \mapsto \pi_\theta$.


\begin{remark}\label{rmk:PolicyGradPseudoLoss} The expression \eqref{eq:MCPolicyGrad} can be used as-is for functions with simple closed-form derivatives.
In an automatic differentiation framework such as {\normalfont\sffamily PyTorch}, we can instead get the policy gradient from a computational graph with the following pseudo-loss function:
\begin{equation}\label{eq:PolicyGradPseudoLoss}
	\tilde{J}(\theta) =
	\frac{1}{M}\sum_{i=1}^M R(\tau_i)
	\sum_{t=0}^{T_i} \log\pi_\theta(s_t^i, a_t^i) =
	\frac{1}{M}\sum_{i=1}^M
	\left(
		\sum_{t=0}^{T_i} \gamma^t r_{t}
	\right)
	\sum_{t=0}^{T_i} \log\pi_\theta(s_t^i, a_t^i)
\end{equation}
\end{remark}


\subsection{Variance reduction: temporal structure and baselines}

We can re-weigh the log-probability gradients in \cref{eq:PolicyGradient} by exploiting the fact that, for any time $t$, the cumulative rewards $\sum_{t'=0}^{t-1} \gamma^{t'} r_{t'}$ from $0$ to $t-1$ are measurable with respect to the trajectory up to $t$, $\tau_{0:t}$:
\begin{prop}
The policy gradient can be rewritten as
\begin{equation}
\nabla_\theta J(\pi_\theta) =
\EE\left[
	\sum_{t=0}^T
	{\redfont
		\sum_{t'=t}^T \gamma^{t'} r_{t'}
	}
	\nabla_\theta \log\pi_\theta(s_t,a_t)
\right]
\end{equation}
which leads to the policy gradient estimate
\begin{equation}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^{T_i} \gamma^t
	{\redfont\hat{q}^i_t}
	\nabla_\theta \log\pi_\theta (s_t^i, a_t^i)
\end{equation}
where\footnote{This quantity can be seen as an estimate of the state-action value function $Q^\pi(s_t, a_t) = \EE[\sum_{t'=t}^T \gamma^{t'-t} r_{t'} \mid s_{t}, a_{t} ]$.} $\hat{q}^i_t = \sum_{t'=t}^T \gamma^{t'-t}r_{t'}$.
\end{prop}


Given any \textbf{\bluefont baseline} function $b\colon \calS \to \RR$, we can rewrite the policy gradient again as
\begin{equation}
	\nabla_\theta J(\pi_\theta) =
	\EE_\pi \left[
	\sum_{t=0}^T
	\left(
		\sum_{t'=t}^T \gamma^{t'} r_{t'} - b(s_t)
	\right)
	\nabla_\theta \log\pi_\theta(s_t, a_t)
	\right]
\end{equation}
The resulting policy gradient estimate we get is
\begin{equation}\label{eq:MCPolicyGradBaseline}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^{T_i}
	\left(
		\hat{q}^i_t - b(s^i_t)
	\right)
	\nabla_\theta \log\pi_\theta (s_t^i, a_t^i)
\end{equation}
which is an unbiased estimate.

It can be shown that the best baseline $b^*$ is the value function:
\[
	b^*(t_0, s) = \EE_\pi\left[\sum_{t=t_0}^T \gamma^{t-t_0} r_t \scalemid s_{t_0} = s\right]
\]
\ldots which we are trying to approximate. This suggests that we use some kind of \textbf{bootstrap} estimate for the baseline.



\subsection{Parametric Bootstrapping of the baseline}

We define the bootstrap estimate $\hat{b} = \hat{v}_\nu(\cdot)$, where $\hat{v}_\nu$ is in a parametric search space with parameter $\nu \in \mathcal{V}$. The parameter can be iteratively updated using gradient steps by alternating with the policy optimization steps.

For a given trajectory sample $\tau=\{s_0,a_0,r_0,\ldots\}$, introduce the mean-squared error between the forward cumulative rewards (a nonparametric estimate of the value function) and the output of the value model:
\[
	\calL(\nu; \tau) =
	\sum_{t=0}^{T} \left(
	\sum_{t'=t}^{T}
	\gamma^{t'-t} r_{t'}
	- \hat{v}_\nu(s_t)
	\right)^2
\]
Then before each update of the policy $\pi_\theta$, update the value parameter $\nu$ using either the gradient of $\calL$.



\section{Parametric approximation: Actor-Critic algorithms}


\paragraph{The idea} The REINFORCE algorithm builds estimates of the $Q$-function to compute the policy gradient as it runs: this is computationally expensive and may lead to high variance. To combat this, it might be a good idea to \textit{learn} from the $Q$-function estimates in a way that gives a consistent estimate that follows the policy gradient updates.

The class of actor-critic methods introduces a second search space for approximation of the state(-action) value function.


\subsection{Actor-critic}


The policy learning is still done by gradient ascent following a policy gradient estimate of the form \cref{eq:MCPolicyGradBaseline} -- but this time, we replace the Monte Carlo estimate $\hat{q}^i_t$ of the $Q$-function by a parametric estimator $\hat{q}_\omega(s_t, a_t)$
\[
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^{T_i} \hat{q}_\omega(s^i_t,a^i_t)
	\nabla_\theta \log\pi_\theta(s^i_t, a^i_t)
\]

\subsection{Actor-critic with baselines: advantage}



\nocite{*}
\printbibliography


\end{document}

