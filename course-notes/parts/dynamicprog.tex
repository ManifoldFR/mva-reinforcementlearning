\documentclass[../course-notes.tex]{subfiles}

\begin{document}


\chapter{Value functions, Dynamic programming}\label{chap:DP}


\section{The value function}

The value function is a staple from the literature on dynamic programming, whether it be for discrete or continuous problems (as in control theory). It measures just how good a control $u$ -- or, in our case, a policy $\pi$ -- is regarding the desired target of our problem.


\begin{defn}[Value function]
	The value function $V^\pi\colon \calS \to \RR$ of a policy $\pi$ is the expectation of the cumulative (discounted) future rewards starting from a point $s_0 = s$
	\begin{equation}\label{eq:ValueFunctionDefn}
	V^\pi(s) \coloneqq
	\EE_{\tau\sim\pi}\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t)
	\scalemid s_0 = s
	\right]
	\end{equation}
	where the trajectory $\tau$ is generated under the policy $\pi$.
	
	$T$ can be a random stopping time corresponding to the first time of arrival at a so-called \emph{terminal} state. It can also be $T = \infty$ for \emph{infinite horizon} problems.
\end{defn}


\begin{remark}[Notational abuse]
	Often, we will write the value functon at a state $s_t$, where it is implied we are at the $t$-th step in a trajectory, as
	\[
	V(s_t) = \EE_{\tau\sim\pi}\left[
	\sum_{{\redfont t'=t}}^T \gamma^{{\redfont t'-t}} r(s_{t'}, a_{t'})
	\scalemid s_t
	\right]
	\]
	This is coherent with the notion of cumulative (discounted) future rewards which defined the value function \eqref{eq:ValueFunctionDefn}, but not with the notation. In the finite horizon setting, a more correct notation would be to write $V(t, s_t)$ and make the dependence on the starting time $t$ of the discounting explicit.
\end{remark}


This notion can be generalized to other cases, such as the case where the rewards are generated by the transitions $(s_t, a_t, s_{t+1})$ rather than the (state, action) couple:
\[
V^\pi(s) = \EE\left[
\sum_{t=0}^{T-1} \gamma^t r(s_t, a_t, s_{t+1})
\scalemid s_0 = s
\right]
\]
or the stochastic reward case where $r_t$ is distributed according to some law parameterized by the state and action.

Under a deterministic policy $\pi\colon \calS \to \calA$ and associated decision rule $d^\pi(s) = \pi(s)$, the dynamic programming principle leads to a dynamic programming equation called the \textbf{\bluefont Bellman equation}:
\begin{equation}\label{eq:ValueBellmanEqn}
V^\pi(s) = r(s, \pi(s)) + \gamma\sum_{s'\in\calS} p(s, \pi(s), s')V^\pi(s')
\end{equation}
which is a fixed-point condition.

The Bellman equation can be used to \textit{evaluate} a policy $\pi$, that is compute its associated value function $V^\pi$.

Its fixed-point structure can be reformulated in terms of an operator on a function space, called the \textbf{Bellman operator}:
\[
\calT^\pi v(s) \coloneqq
r(s,\pi(s)) + \gamma\sum_{s'\in\calS}p(s,\pi(s),s') v(s')
\]
which can be shown to be contractant, ensuring the existence of a solution.


\begin{remark}
	We have the following, possible generalizations (see Sutton's book \cite[][chap.\ 3,4]{Sutton1998} for further details):
	\begin{itemize}
		\item for stochastic policies $\pi\colon \calS\times\calA \to \RR_+$, the sum in \cref{eq:ValueBellmanEqn} becomes 
		\[
		\sum_{s'\in\calS} \sum_{a\in\calA} \pi(s,a) p(s,a,s')V^\pi(s')
		\]
		\item in non-discrete state spaces, the sum can be replaced by an integral with respect to a measure $p(s,\pi(s),\der s')$
		\item if the rewards are given for transitions as $r(s,a,s')$, we introduce
		\[
			r(s,a) = \sum_{s'\in\calS} p(s,a,s') r(s,a,s'),
		\]
		and the Bellman equation can be rewritten
		\[
		V^\pi(s) = \sum_{s'\in\calS} p(s,\pi(s),s')
		(r(s,\pi(s),s') + \gamma V^\pi(s'))
		\]
	\end{itemize}
\end{remark}


In a finite setting with small state and action spaces, it possible to solve the Bellman equation directly by Gaussian elimination on the corresponding matrices and vectors.



\subsection{The optimal value function}\label{sec:OptimalValueFunc}

Solving a Markov Decision Process involves finding an \textbf{\bluefont optimal} policy $\pi^*$ that will maximize the expected rewards in the long run when starting from a given state $s_0$ (or distribution $s_0 \sim p$).

\begin{defn}[Optimal policy and value function]
	Given a set of policies $\Pi$, the \emph{\bfseries\bluefont optimal value function} satisfies
	\begin{equation}
	V^* = \max_{\pi} V^{\pi}
	\end{equation}
	for every state $s \in \calS$. An \emph{\bfseries\bluefont optimal policy} $\pi^*$ is one that satisfies the maximum.
\end{defn}

Strictly speaking, we are taking a maximal policy with respect to a partial ordering on policies that compares them by looking at their respective value functions:
\[
	\pi_1 \leq \pi_2 \Longleftrightarrow
	\left[
	V^{\pi_1}(s) \leq V^{\pi_2}(s)\ \forall s\in\calS
	\right]
\]


\begin{prop}[Optimal Bellman equation]
	The optimal value function $V^*$ obeys a dynamic programming principle, the \emph{\bfseries\bluefont optimal Bellman equation}.
	\begin{equation}\label{eq:ValueOptimalBellmanEq}
	V^*(s) = \max_{a\in\calA}
	\left\{
	r(s,a) + \sum_{s'\in\calS} p(s,a,s')V^*(s')
	\right\}
	\end{equation}
\end{prop}


This is also a fixed-point condition, which can once again be expressed in terms of an operator called the \textbf{Bellman optimal operator}:
\[
\calT^*v(s) \coloneqq
\max_{a\in\calA} \left\{
r(s,a) + \gamma\sum_{s'\in\calS}p(s,a,s')v(s')
\right\}.
\]
Then, we have that the optimal value function satisfies the equation
\[
\calT^* V^* = V^*
\]


\section{The $Q$-function}



\begin{defn}[action-state value function]
	The \textit{action-state value function} of a policy $\pi$ is the function $Q^\pi\colon \calS\times\calA\to \RR$ is defined by
	\begin{equation}
	Q^\pi(s, a) \coloneqq \EE_{\pi}
	\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t) \scalemid s_0 = s, a_0 = a
	\right]
	\end{equation}
	where the trajectory $\tau$ is generated under the policy $\pi$.
	The horizon $T$ of the problem can be finite or infinite ($T$ can be a stopping time).
\end{defn}



The action-state value function $Q^\pi$ has an obvious link to the value function $V^\pi$.
For any state $s\in\calS$, it holds that
\begin{equation}\label{eq:ValueFunctionExpectationLink}
	V^\pi(s) = \EE_{a\sim \pi(s,\cdot)}\left[
	Q^\pi(s, a)
	\right]
	= \sum_a \pi(s,a) Q^\pi(s,a)
\end{equation}

There's also a reverse equality:
\begin{equation}
	Q^\pi(s,a) =
	\EE_{\pi}\left[
		r(s,a) + V^\pi(s_{t+1})
		\scalemid s_t = s
	\right]
	=
	r(s, a) + \sum_{s'} p(s,a,s') V^\pi(s')
\end{equation}

The action-state value function has its own \textbf{\bluefont Bellman equation}:
\begin{equation}
	Q^\pi(s,a) = r(s,a) + \gamma
	\sum_{s'} p(s,a,s') Q^\pi(s',a)
\end{equation}


\subsection{The optimal $Q$-function}

\begin{defn}[Optimal action value function]
	Optimal policies as defined in \cref{sec:OptimalValueFunc} also share the same \emph{\bfseries\bluefont optimal action-state value function}
	\begin{equation}
		Q^* = \max_\pi Q^\pi
	\end{equation}
	for all states $s\in\calS$ and actions $a\in\calA$.	
\end{defn}

Here, the maximum is also understood with respect to a partial ordering on policies.

The optimal value $Q^*$ follows its own variant of the \textbf{\bluefont optimal Bellman equation}:
\begin{equation}\label{eq:QValOptimalBellmanEqn}
	Q^*(s,a) = r(s,a) + 
	\gamma\sum_{s'\in\calS} p(s,a,s') \max_{a'} Q^*(s',a')
\end{equation}


\subsubsection{Link between the optimal value functions}

We also have a link between the optimal value functions. Using the Bellman optimality equation on $Q$ \eqref{eq:QValOptimalBellmanEqn}, we get:
\[
	Q^*(s,a) = \EE_\pi\left[
	r_t + \gamma V^*(s_{t+1}) \scalemid s_t=s, a_t=a
	\right]
\]
Conversely, the optimal value function can be expressed by reformulating the Bellman optimality equation \eqref{eq:ValueOptimalBellmanEq}:
\begin{equation}
	V^*(s) =
	\argmax_{a\in\calA} Q^*(s,a)
\end{equation}


\subsubsection{Obtaining the optimal policy}

Given the optimal action-state value function $Q^*$, you can define an optimal policy by taking
\[
	\pi^*(s) \in \argmax_{a\in\calA} Q^*(s,a).
\]
By the way, this shows we can always find a \underline{deterministic} optimal policy. This is an \textbf{important} property of the optimal $Q$-function which we will use to design policy optimization algorithms.


\section{Algorithms for dynamic programming}



\subsection{Value Iteration}

Under suitable hypotheses, it can be shown that the Bellman operators $\calT^\pi$ and $\calT^*$ are contractions with respect to the infinity norm with Lipschitz constant $\gamma$.

This means we can approximate the optimal value function by iterating the optimal Bellman operator $\calT^*$. This leads to the following \textit{value iteration} algorithm:

\begin{algorithm}
	\caption{Value iteration}
	\KwIn{Rewards $r(s,a)$, transitions $p(s,a,s')$, initial value proposal $V_0$, number of iterations $K$}
	\ForEach{$k=1,\ldots,K$}{
		$V_k \leftarrow \calT^* V_{k-1}$\;
	}
	\ForEach{$s\in\calS$}{
		$\pi_K(s) \leftarrow \argmax_{a\in\calA} \left\{r(s,a)+\gamma\sum_{s'\in\calS}p(s,a,s')V_K(s')\right\}$\;
	}
	\Return Policy $\pi_K$, value $V_K$\;
\end{algorithm}

\begin{remark}
Each iterate $V_k$ is not necessarily the value function of some policy.
\end{remark}

This algorithm is especially useful for discrete state and action spaces. It can be significantly sped up if we have sparse representations of the rewards and transitions.

The stopping condition for the iterations can either be that a fixed number of iterations $K$ has been reached, or an $\epsilon$-convergence criterion, i.e. stop when value function increments fall below some threshold (e.g. $\|V_{k+1} - V_k\|_{\infty} \leq \epsilon$).

\begin{prop}[Convergence speed of VI]
	If given an upper bound $r_{\max{}} \geq \sup |r(\cdot)|$ on the rewards, we $\epsilon$-converge in at most
	\[
		K = \frac{\log(r_{\max{}})/\epsilon}{\log(1/\gamma)}
	\]
	steps.
\end{prop}

\paragraph{Performance guarantees} The performance loss between the final policy $\pi_K$ and the optimal policy $\pi^*$ is measured by the distance between their values:
\[
	\|V^* - V^{\pi_K}\|_\infty \leq
	\frac{2\gamma}{1 - \gamma} \| V^* - V_K\|_\infty
\]

\paragraph{Alternative termination condition}
\[
	\|V_k-V_{k-1}\|_\infty -
	\min_{s\in\calS} |V_k(s) - V_{k-1}(s)| \leq \epsilon
\]

\paragraph{Complexity} The temporal complexity of VI is $O(KS^2A)$ -- indeed the complexity of applying the Bellman operator is $O(S^2A)$



\subsubsection{Variants}






\subsection{Policy Iteration}

The idea of \textit{policy iteration} is to use the action-state value function $Q^\pi$ to iteratively update the policy.

\paragraph{Greedy improvements} We can improve over a policy $\pi$ and define a new policy $\pi'$ by acting greedily:
\[
	\pi'(s) \coloneqq \argmax_{a\in\calA} Q^\pi(s,a)
\]
It can then be shown that $V^{\pi'} \geq V^{\pi}$, and that improvements stop if and only if the Bellman optimality condition is satisfied.

To decide when to stop, we can use a set number of iterations, or use a $\epsilon$-convergence criterion as suggested before for value iteration.


Policy iteration can also be performed for other improvement algorithms. For instance, \textbf{$\boldsymbol\epsilon$-greedy improvement} performs greedy updates at each state probabilistically, taking the greedy with probability $1-\epsilon$ and otherwise taking a random action.




\chapter{Approximate Reinforcement Learning}\label{chap:ApproxRL}

As in regular control theory, dynamic programming opens up a lot of venues to solve MDPs we know everything about. But what do we do when we don't have a full, tabular model of the transitions and rewards of our system?

There are two ideas: use several \textbf{\bluefont episodes} (trajectories) of agents following some policy (or policies) to compute empirical returns and build estimates of the value, or update our estimates as-we-go as we follow an agent's actions and the rewards it gets (\textbf{\bluefont online learning}).


There are two aspects that need to be discussed:
\begin{itemize}
	\item \textbf{policy evaluation}
	\item \textbf{policy optimization}
\end{itemize}


We will start by discussing approximate policy evaluation techniques and see how they apply to optimizing the policies as we go.


\section{Monte Carlo Policy Evaluation}

The idea is naïve: generate episodes $\tau_i$ under a policy $\pi$ starting at some state $s_0$, and compute their empirical returns
\[
	R(\tau_i) = \sum_{t=0}^{T_i} \gamma^t r_{t,i}.
\]
The value estimate is then the empirical mean:
\begin{equation}
	\widehat{V}^\pi(s_0) = \frac{1}{n}\sum_{i=1}^{n} R(\tau_i)
\end{equation}

Here, the beginning state $s_0$ is fixed so we are only estimating its value.


\paragraph{Dealing with non-episodic problems} If the underlying problem does not necessarily terminate in finite time, we can always truncate it after a given number of time steps and say it has indeed ``reset". For instance, a trade execution algorithm does not have a terminal state so we might want to generate episodes by resetting after some time has passed. This means ignoring a term $\sum_{t'=H+1}^\infty \gamma^{t'} r_{t'}$ in the return past a certain horizon $H$. In that case, the MC estimator converges to a truncated value function $V_H^\pi$ which differs from the true value $V^\pi$ by
\[
	|V_H^\pi(s_0) - V^\pi(s_0)| \leq \gamma^H\frac{\| r(\cdot)\|_\infty}{1 - \gamma}.
\]


\subsection{Incremental MC}

We can easily see that the $n$-sample MC estimate can be seen as an update of the $(n-1)$-sample one:
\[
	\widehat{V}_{n}^\pi(s_0) = \alpha_n R(\tau_{n}) + (1-\alpha_n) \widehat{V}^\pi_{n-1}(s_0)
\]
where
\[
	\alpha_n = \frac{1}{n}
\]
is the \textbf{learning rate} of the scheme.

Other learning rates can be used, and we have the following result ensuring convergence in the general case:
\begin{prop}
	Suppose the learning rate $(\alpha_n)$ satisfies the Robbins-Monro condition:
	\begin{equation}
	\sum_{n=0}^{\infty} \alpha_n = \infty \quad
	\sum_{n=0}^{\infty} \alpha_n^2 < \infty
	\end{equation}
	then the incremental MC estimate converges to the real value:
	\[
		\widehat{V}^\pi(s_0) \xrightarrow[n\to\infty]{} V^\pi(s_0).
	\]
\end{prop}

Incremental MC is also called $\TD(1)$. It does not allow online learning.



\section{Temporal difference (TD) estimation}

\subsection{Using the Bellman equation: $\mathsf{TD}(0)$ estimation}

The real value function $V^\pi$ satisfies the Bellman equation. This means that the \textbf{\bluefont temporal difference error} of a good estimate $\widehat{V}^\pi$ of $V^\pi$, defined after each transition and reward ${\redfont s_t,r_t,s_{t+1}}$
\begin{equation}\label{eq:ValueTDerror}
	{\bluefont\delta_t} = {\redfont r_t} + \gamma \widehat{V}^\pi({\redfont s_{t+1}}) - \widehat{V}^\pi({\redfont s_t}),
\end{equation}
should be small.

The $\TD(0)$ updates are performed as
\begin{align*}
	\widehat{V}^\pi(s_t) &\leftarrow
	\widehat{V}^\pi(s_t) + \alpha_t {\bluefont \delta_t} \\
	&= (1 - \alpha_t)\widehat{V}^\pi({\redfont s_t}) + \alpha_t ({\redfont r_t} + \gamma \widehat{V}^\pi({\redfont s_{t+1}}))
\end{align*}

\paragraph{Choosing the learning rate} For incremental MC, a natural learning rate that works was $\alpha_n = 1/n$. For this temporal difference update scheme, it can be shown that we can use an \textbf{adaptive state-dependent learning rate} $\alpha_t = \alpha(s_t)$:
\begin{prop}\label{prop:ValueTD0learningRate}
	Set the learning rate to $\alpha_t = \alpha(N_t(s_t))$ where $N_t(s)$ is the number of times state $s\in\calS$ has been visited before time $t$. If we suppose $N_t(s) \to \infty$ (every state is visited infinitely many times), then the conditions
	\begin{equation}
	\sum_{n=0}^{\infty} \alpha(n) = \infty \quad
	\sum_{n=0}^{\infty} \alpha(n)^2 < \infty
	\end{equation}
	lead to convergence of the scheme.
\end{prop}

This scheme doesn't require the entire trajectory to be known to perform an update, allowing for fully online learning.



\subsection{Temporal difference $\TD(\lambda)$}

These schemes average between incremental MC and $\TD(1)$. The scheme is defined for $0 < \lambda < 1$ as:
\begin{equation}
	\widehat{V}^\pi(s_t) \leftarrow \widehat{V}^\pi(s_t) + \alpha(s_t) \sum_{t'=t}^T (\gamma\lambda)^{t'-t}\delta_{t'}
\end{equation}
where ${(\delta_{t'})}_{t'}$ is the sequence of temporal differences errors of the trajectory.


This scheme \textit{does} require the entire trajectory to be known before performing updates, but it re-discounts them using the parameter $\lambda$.


\subsubsection{Eligibility traces}

At every transition $s_t,r_t,s_{t+1}$, we define the temporal difference $\delta_t$ and update the \textbf{\bluefont eligibility traces} {\bluefont $z(s)$} as
\[
	\forall s\in\calS \quad 
	z(s) = \begin{cases}
	\lambda z(s) &\text{if }s\neq s_t \\
	1 + \lambda z(s)& \text{if }s=s_t \\
	0& \text{if } s_t = s_0 \text{ (reset the traces)}
	\end{cases}
\]

For policy evaluation, we update the value function estimate for all states $s\in\calS$ using the eligibility traces with
\begin{equation}
	\widehat{V}^\pi(s) \leftarrow
	\widehat{V}^\pi(s) + \alpha(s){\bluefont z(s)}\delta_t
\end{equation}



This introduces a compromise between propagating rewards faster than $\TD(0)$ and having smaller variance than incremental MC.


\begin{remark}
	All the previous ideas for policy evaluation on the value function $V^\pi$ readily apply to evaluating the action-state value $Q^\pi$.
\end{remark}



\section{Application of TD to policy improvement}

After introducing the previous temporal difference notions for policy evaluation, we can see how they work for \textbf{policy improvement} schemes. The basic idea is to alternate the previous temporal-difference policy evaluation techniques with policy improvement ideas from \cref{chap:DP} which exploit the $Q$ value function.

\subsection{SARSA}

We define an exploration policy using the softmax function and a temperature parameter $\tau$:
\[
	\pi_Q(s,a) =
	\frac{\exp(Q(s,a)/\tau)}{
		\sum_{a'\in\calA} \exp(Q(s,a')/\tau)
	}
\]
After taking an action according to $\pi_Q$, we update using the $\TD(1)$ scheme: compute the temporal difference of the $Q$-function
\begin{equation}\label{eq:QfuncTDerror}
	{\bluefont \delta_t} = r_t + \gamma \widehat{Q}(s_{t+1}, a_{t+1}) - \widehat{Q}(s_t, a_t)
\end{equation}
and update $Q$
\begin{equation*}
\begin{aligned}
	\widehat{Q}(s_t, a_t) &\leftarrow
	\widehat{Q}(s_t, a_t) + \alpha_t {\bluefont \delta_t}  \\
	&= (1-\alpha_t)\widehat{Q}(s_t, a_t)
	+ \alpha_t (r_t + \widehat{Q}(s_{t+1}, a_{t+1}))
\end{aligned}
\end{equation*}

We can ensure the policy continues to improve by decreasing the temperature $\tau$ to 0 and becoming more and more greedy in our action selection (keeping the same $\tau$ reduces the algorithm to some kind of softmax policy evaluation).

Extension to $\TD(\lambda)$ update schemes is straightforward.



\subsection{Q-learning}

The idea is to introduce the optimal TD error for the $Q$ value function
\begin{equation}
	\delta_t = r_t + \gamma \max_{a'}\widehat{Q}(s_{t+1},a') - \widehat{Q}(s_t,a_t)
\end{equation}
and updating the $Q$ estimate using that:
\[
	\widehat{Q}(s_t,a_t) \leftarrow \widehat{Q}(s_t,a_t) + \alpha_t {\bluefont\delta_t}
\]

\begin{remark}
	This is \underline{not} the same as taking the greedy action when simulating the trajectory: this is upper-bounding the $Q$-value TD error \eqref{eq:QfuncTDerror} and using that upper bound to update the value.
\end{remark}


\begin{algorithm}
	\caption{Q-learning}
	\KwIn{Initial state $s_0$, exploration policy}
	\Repeat{trajectory terminates}{
		Take action $a_t$ according the exploration policy\;
		Get reward and next state $(r_t, s_{t+1})$\;
		Compute TD error $\delta_t = r_t + \gamma\max_{a'}\widehat{Q}(s_{t+1},a') - \widehat{Q}(s_t, a_t)$\;
		$\widehat{Q}(s_t,a_t) \leftarrow \widehat{Q}(s_t,a_t) + \alpha_t \delta_t$\;
	}
	\tcp{Compute the policy from the $Q$ value}
	\ForEach{$s\in\calS$}{
		$\tilde{\pi}(s) \leftarrow \argmax_{a\in\calA} \widehat{Q}(s,a)$\;
	}
	\Return Policy $\tilde{\pi}$\;
\end{algorithm}

We can use several exploration policies, such as $\epsilon$-greedy exploration (where we take the $\widehat{Q}$-optimal action with probability $\epsilon$ and otherwise take a random action).


\paragraph{What about the learning rate} We can introduce, just as in evaluating the value $V$ (see \cref{prop:ValueTD0learningRate}), a action-state count $N_t(s,a)$ and set an adaptive learning rate
\[
	\alpha_t = \alpha(N_t(s_t, a_t))
\]
and the Robbins-Monro conditions on the function $\alpha$ work out the same.



\end{document}
