\documentclass[../course-notes.tex]{subfiles}

\begin{document}


\chapter{Value functions, Dynamic programming}


\section{The value function}

The value function is a staple from the literature on dynamic programming, whether it be for discrete or continuous problems (as in control theory). It measures just how good a control $u$ -- or, in our case, a policy $\pi$ -- is regarding the desired target of our problem.


\begin{defn}[Value function]
	The value function $V^\pi\colon \calS \to \RR$ of a policy $\pi$ is the expectation of the cumulative (discounted) future rewards starting from a point $s_0 = s$
	\begin{equation}\label{eq:ValueFunctionDefn}
	V^\pi(s) \coloneqq
	\EE_{\tau\sim\pi}\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t)
	\scalemid s_0 = s
	\right]
	\end{equation}
	where the trajectory $\tau$ is generated under the policy $\pi$.
	
	$T$ can be a random stopping time corresponding to the first time of arrival at a so-called \emph{terminal} state. It can also be $T = \infty$ for \emph{infinite horizon} problems.
\end{defn}


\begin{remark}[Notational abuse]
	Often, we will write the value functon at a state $s_t$, where it is implied we are at the $t$-th step in a trajectory, as
	\[
	V(s_t) = \EE_{\tau\sim\pi}\left[
	\sum_{{\redfont t'=t}}^T \gamma^{{\redfont t'-t}} r(s_{t'}, a_{t'})
	\scalemid s_t
	\right]
	\]
	This is coherent with the notion of cumulative (discounted) future rewards which defined the value function \eqref{eq:ValueFunctionDefn}, but not with the notation. In the finite horizon setting, a more correct notation would be to write $V(t, s_t)$ and make the dependence on the starting time $t$ of the discounting explicit.
\end{remark}


This notion can be generalized to other cases, such as the case where the rewards are generated by the transitions $(s_t, a_t, s_{t+1})$ rather than the (state, action) couple:
\[
V^\pi(s) = \EE\left[
\sum_{t=0}^{T-1} \gamma^t r(s_t, a_t, s_{t+1})
\scalemid s_0 = s
\right]
\]
or the stochastic reward case where $r_t$ is distributed according to some law parameterized by the state and action.

Under a deterministic policy $\pi\colon \calS \to \calA$ and associated decision rule $d^\pi(s) = \pi(s)$, the dynamic programming principle leads to a dynamic programming equation called the \textbf{\bluefont Bellman equation}:
\begin{equation}\label{eq:ValueBellmanEqn}
V^\pi(s) = r(s, \pi(s)) + \gamma\sum_{s'\in\calS} p(s, \pi(s), s')V^\pi(s')
\end{equation}
which is a fixed-point condition.

The Bellman equation can be used to \textit{evaluate} a policy $\pi$, that is compute its associated value function $V^\pi$.

Its fixed-point structure can be reformulated in terms of an operator on a function space, called the \textbf{Bellman operator}:
\[
\calT^\pi v(s) \coloneqq
r(s,\pi(s)) + \gamma\sum_{s'\in\calS}p(s,\pi(s),s') v(s')
\]
which can be shown to be contractant, ensuring the existence of a solution.


\begin{remark}
	We have the following, possible generalizations (see Sutton's book \cite[][chap.\ 3,4]{Sutton1998} for further details):
	\begin{itemize}
		\item for stochastic policies $\pi\colon \calS\times\calA \to \RR_+$, the sum in \cref{eq:ValueBellmanEqn} becomes 
		\[
		\sum_{s'\in\calS} \sum_{a\in\calA} \pi(s,a) p(s,a,s')V^\pi(s')
		\]
		\item in non-discrete state spaces, the sum can be replaced by an integral with respect to a measure $p(s,\pi(s),\der s')$
		\item if the rewards are given for transitions as $r(s,a,s')$, we introduce $r(s,a) = \sum_{s'\in\calS}r(s,a,s')$, and the Bellman equation can be rewritten
		\[
		V^\pi(s) = \sum_{s'\in\calS}
		r(s,\pi(s),s') + \gamma p(s,\pi(s),s')V^\pi(s')
		\]
	\end{itemize}
\end{remark}


In a finite setting with small state and action spaces, it possible to solve the Bellman equation directly by Gaussian elimination on the corresponding matrices and vectors.



\subsection{The optimal value function}\label{sec:OptimalValueFunc}

Solving a Markov Decision Process involves finding an \textbf{\bluefont optimal} policy $\pi^*$ that will maximize the expected rewards in the long run when starting from a given state $s_0$ (or distribution $s_0 \sim p$).

\begin{defn}[Optimal policy and value function]
	Given a set of policies $\Pi$, the \emph{\bfseries\bluefont optimal value function} satisfies
	\begin{equation}
	V^* = \max_{\pi} V^{\pi}
	\end{equation}
	for every state $s \in \calS$. An \emph{\bfseries\bluefont optimal policy} $\pi^*$ is one that satisfies the maximum.
\end{defn}

Strictly speaking, we are taking a maximal policy with respect to a partial ordering on policies that compares them by looking at their respective value functions:
\[
	\pi_1 \leq \pi_2 \Longleftrightarrow
	\left[
	V^{\pi_1}(s) \leq V^{\pi_2}(s)\ \forall s\in\calS
	\right]
\]


\begin{prop}[Optimal Bellman equation]
	The optimal value function $V^*$ obeys a dynamic programming principle, the \emph{\bfseries\bluefont optimal Bellman equation}.
	\begin{equation}\label{eq:ValueOptimalBellmanEq}
	V^*(s) = \max_{a\in\calA}
	\left\{
	r(s,a) + \sum_{s'\in\calS} p(s,a,s')V^*(s')
	\right\}
	\end{equation}
\end{prop}


This is also a fixed-point condition, which can once again be expressed in terms of an operator called the \textbf{Bellman optimal operator}:
\[
\calT^*v(s) \coloneqq
\max_{a\in\calA} \left\{
r(s,a) + \gamma\sum_{s'\in\calS}p(s,a,s')v(s')
\right\}.
\]
Then, we have that the optimal value function satisfies the equation
\[
\calT^* V^* = V^*
\]


\section{The $Q$-function}



\begin{defn}[State-action value function]
	The \textit{state-action value function} of a policy $\pi$ is the function $Q^\pi\colon \calS\times\calA\to \RR$ is defined by
	\begin{equation}
	Q^\pi(s, a) \coloneqq \EE_{\tau\sim \pi}
	\left[
	\sum_{t=0}^T \gamma^t r(s_t, a_t) \scalemid s_0 = s, a_0 = a
	\right]
	\end{equation}
	where the trajectory $\tau$ is generated under the policy $\pi$.
	The horizon $T$ of the problem can be finite or infinite ($T$ can be a stopping time).
\end{defn}



The state-action value function $Q^\pi$ has an obvious link to the value function $V^\pi$.
For any state $s\in\calS$, it holds that
\begin{equation}\label{eq:ValueFunctionExpectationLink}
	V^\pi(s) = \EE_{a\sim \pi(s,\cdot)}\left[
	Q^\pi(s, a)
	\right]
	= \sum_a \pi(s,a) Q^\pi(s,a)
\end{equation}

There's also a reverse equality:
\begin{equation}
	Q^\pi(s,a) =
	\EE_{\pi}\left[
		r(s,a) + V^\pi(s_{t+1})
		\scalemid s_t = s
	\right]
	=
	r(s, a) + \sum_{s'} p(s,a,s') V^\pi(s')
\end{equation}

The action-state value function has its own \textbf{\bluefont Bellman equation}:
\begin{equation}
	Q^\pi(s,a) = r(s,a) + \gamma
	\sum_{s'} p(s,a,s') Q^\pi(s',a)
\end{equation}


\subsection{Optimal $Q$-function}

\begin{defn}[Optimal action value function]
	Optimal policies as defined in \cref{sec:OptimalValueFunc} also share the same \emph{\bfseries\bluefont optimal action value function}
	\begin{equation}
	Q^*(s,a) = \max_\pi Q^\pi(s,a)
	\end{equation}
	for $s\in\calS$, $a\in\calA$.	
\end{defn}

Here, the maximum is also understood with respect to a partial ordering on policies.

The optimal value $Q^*$ follows its own variant of the \textbf{\bluefont optimal Bellman equation}:
\begin{equation}\label{eq:QValOptimalBellmanEqn}
	Q^*(s,a) = r(s,a) + 
	\gamma\sum_{s'\in\calS} p(s,a,s') \max_{a'} Q^*(s',a')
\end{equation}


\paragraph{Link between the optimal value functions} We also have a link between the optimal value functions. Using the Bellman optimality equation on $Q$ \eqref{eq:QValOptimalBellmanEqn}, we get:
\[
	Q^*(s,a) = \EE_\pi\left[
	r_t + \gamma V^*(s_{t+1}) \scalemid s_t=s, a_t=a
	\right]
\]
Conversely, the optimal value function can be expressed by reformulating the Bellman optimality equation \eqref{eq:ValueOptimalBellmanEq}:
\begin{equation}
	V^*(s) =
	\argmax_{a\in\calA} Q^*(s,a)
\end{equation}


\paragraph{Obtaining the optimal policy} Given the optimal action-state value function $Q^*$, you can define an optimal policy by taking
\[
	\pi^*(s,a) \in \argmax_{a\in\calA} Q^*(s,a).
\]
By the way, this shows we can always find a \underline{deterministic} optimal policy.


\section{Algorithms for dynamic programming}



\subsection{Value Iteration}

Under suitable hypotheses, it can be shown that the Bellman operators $\calT^\pi$ and $\calT^*$ are contractions with respect to the infinity norm with Lipschitz constant $\gamma$.

This means we can approximate the optimal value function by iterating the optimal Bellman operator $\calT^*$. This leads to the following \textit{value iteration} algorithm:

\begin{algorithm}
	\caption{Value iteration}
	\KwIn{Rewards $r(s,a)$, transitions $p(s,a,s')$, initial value proposal $V_0$, number of iterations $K$}
	\ForEach{$k=1,\ldots,K$}{
		$V_k \leftarrow \calT^* V_{k-1}$\;
	}
	\ForEach{$s\in\calS$}{
		$\pi_K(s) \leftarrow \argmax_{a\in\calA} \left\{r(s,a)+\gamma\sum_{s'\in\calS}p(s,a,s')V_K(s')\right\}$\;
	}
	\Return Policy $\pi_K$, value $V_K$\;
\end{algorithm}

This algorithm is especially useful for discrete state and action spaces. It can be significantly sped up if we have sparse representations of the rewards and transitions.

Each iterate $V_k$ is not necessarily the value function of some policy.

The stopping condition for the iterations can either be that a fixed number of iterations $K$ has been reached, or that the value function increments have fallen below some threshold (e.g. that $\|V_{k+1} - V_k\|_{\infty} \leq \epsilon$ for instance).





\subsection{Policy Iteration}

The idea of \textit{policy iteration} is to use the action-state value function $Q^\pi$ to iteratively update the policy.

\paragraph{Greedy improvements} We can improve over a policy $\pi$ and define a new policy $\pi'$ by acting greedily:
\[
	\pi'(s) \coloneqq \argmax_{a\in\calA} Q^\pi(s,a)
\]
It can then be shown that $V^{\pi'} \geq V^{\pi}$, and that improvements stop if and only if the Bellman optimality condition is satisfied.

To decide when to stop, we can use a set number of iterations, or use $\epsilon$-convergence criterion as suggested before for value iteration.


Policy iteration can also be performed for other improvement algorithms. For instance, \textbf{$\boldsymbol\epsilon$-greedy improvement} performs greedy updates at each state probabilistically.





\chapter{Approximate Reinforcement Learning}

As in regular control theory, dynamic programming opened up a lot of venues to solve MDPs we know everything about. But what do we do when we don't have a full, tabular model of the transitions, rewards\ldots~of our system?

There are two ideas: use several \textbf{\bluefont episodes} (trajectories) of agents following some policy (or policies) to compute empirical returns and build estimates of the value, or update our estimates as-we-go as we follow an agent's actions and the rewards it gets (\textbf{\bluefont online learning}).






\section{Monte Carlo}

The idea is naïve: generate episodes $\tau_i$ under a policy $\pi$ starting at some state $s_0$, and compute their empirical returns
\[
	R(\tau_i) = \sum_{t=0}^{T_i} \gamma^t r_{t,i}.
\]
The value estimate is then the empirical mean:
\begin{equation}
	\widehat{V}^\pi(s_0) = \frac{1}{n}\sum_{i=1}^{n} R(\tau_i)
\end{equation}

Here, the beginning state $s_0$ is fixed so we are only estimating its value.


\paragraph{Dealing with non-episodic problems} If the underlying problem does not necessarily terminate in finite time, we can always truncate it after a given number of time steps and say it has indeed ``reset". For instance, a trade execution algorithm does not have a terminal state so we might want to generate episodes by resetting after some time has passed. This means ignoring a term $\sum_{t'=H+1}^\infty \gamma^{t'} r_{t'}$ in the return past a certain horizon $H$. In that case, the MC estimator converges to a truncated value function $V_H^\pi$ which differs from the true value $V^\pi$ by
\[
	|V_H^\pi(s_0) - V^\pi(s_0)| \leq \gamma^H\frac{\| r(\cdot)\|_\infty}{1 - \gamma}.
\]


\subsection{Incremental MC}

We can easily see that the $n$-sample MC estimate can be seen as an update of the $(n-1)$-sample one:
\[
	\widehat{V}_{n}^\pi(s_0) = \alpha_n R(\tau_{n}) + (1-\alpha_n) \widehat{V}^\pi_{n-1}(s_0)
\]
where
\[
	\alpha_n = \frac{1}{n}
\]
is the \textbf{learning rate} of the scheme.

Other learning rates can be used, and we have the following result ensuring convergence in the general case:
\begin{prop}
	Suppose the learning rate $(\alpha_n)$ satisfies the Robbins-Monro condition:
	\begin{equation}
	\sum_{n=0}^{\infty} \alpha_n = \infty \quad
	\sum_{n=0}^{\infty} \alpha_n^2 < \infty
	\end{equation}
	then the incremental MC estimate converges to the real value:
	\[
		\widehat{V}^\pi(s_0) \xrightarrow[n\to\infty]{} V^\pi(s_0).
	\]
\end{prop}

Incremental MC is also called $\TD(1)$.




\section{Using the Bellman equation -- $\mathsf{TD}(0)$ estimation}

The real value function $V^\pi$ satisfies the Bellman equation. This means that the \textbf{\bluefont temporal difference error} of a good estimate $\widehat{V}^\pi$ of $V^\pi$, defined as
\[
	\delta_t = r_t + \gamma \widehat{V}^\pi(s_{t+1}) - \widehat{V}^\pi(s_t),
\]
should be small.






\section{$\TD(\lambda)$}

\subsection{Eligibility traces}







\end{document}