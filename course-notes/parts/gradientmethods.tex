\documentclass[../course-notes.tex]{subfiles}



\begin{document}

\chapter{Approximate solving of MDPs with Policy Gradients}


Solving MDPs is seeking the maximizing policy of the value function. For approximate solving of MDPs, we target what could be a more general \textbf{\bluefont policy performance metric}. Often, it is indeed the value function
\begin{equation}\label{eq:PolicyPerfCumReward}
J(\pi) = \EE_{\tau\sim\pi}\left[
\sum_{t=0}^{T} \gamma^t r_t
\right] =
\EE_{\tau\sim\pi}
\left[R(\tau)\right]
\end{equation}
where $\tau = \left\{ s_1,a_1,r_1,\ldots,s_{T-1},a_{T-1},r_{T-1},s_{T}\right\}$ and $R(\tau) = \sum_{t=0}^T \gamma^t r_t$ is the total return of the trajectory $\tau$.
We use the shorthand $r_t$ for the reward $r(s_t, a_t)$ -- or more generally a transition reward $r(s_t, a_t, s_{t+1})$ or stochastic reward distributed as $r_t \sim p(\cdot, s_t, a_t, s_{t+1})$. The expectation $J$ is either conditional on a given starting point $s_0$, or on a distribution for it.\footnote{For instance, OpenAI Gym's \texttt{CartPole-v1} environment has a stochastic initial state $s_0$.}

We seek to compute the maximizing policy in a parametric search space $\{\pi_\theta: \theta\in\Theta\}$:
\[
\max_\theta J(\pi_\theta)
\]

Iterative methods could be used if $J$ can be obtained in closed form; if these are not given they could be estimated by Monte Carlo methods, which would be very expensive and wasteful.

Instead, we will update the estimate as we go and simulate trajectories, by iteratively updating the policy parameter $\theta$ using a gradient ascent method with an estimated gradient.

\begin{prop}[Gradient under a parametric law]\label{prop:parametricGradient}
	Given a set of probability models $\{P_\theta: \theta\in\Theta\subseteq \RR^d\}$ on a set $\calX$ and a function $f\colon \calX\to \RR$, we have that
	\[
	\nabla_\theta \EE_{X\sim P_\theta}[f(X)]
	= \EE_{X\sim P_\theta}[f(X)\nabla_\theta \log P_\theta(X)]
	\]
	This is a useful property for deriving estimators of the derivatives in optimization problems with stochastic objectives.
	
	Generalization to the case where $f$ also depends on $\theta$ is straightforward.
\end{prop}

This can be shown either by either writing the expectation as an integral, or by a change of measures with a Radon-Nikodym derivative.

\Cref{prop:parametricGradient} allows us to write the gradient of \eqref{eq:PolicyPerfCumReward}, called the \textbf{\bluefont policy gradient} as an expectation:
\begin{equation}\label{eq:PolicyGradient}
\nabla_\theta J(\pi_\theta) =
\EE_{\tau\sim\pi_\theta}\left[
R(\tau)\sum_{t=0}^T\nabla_\theta \log\pi_\theta(s_t, a_t)
\right]
\end{equation}
and we will need to derive estimations for this quantity.

There are other ways of writing the policy gradient, such as (see \cite[][chap.\ 13]{Sutton1998})
\begin{align*}
\nabla_\theta J(\pi_\theta)
&=
\EE_s\left[
\sum_a Q^{\pi_\theta}(s,a) \nabla\pi_\theta(s,a)
\right]
\end{align*}




\section{Monte Carlo policy gradient: the REINFORCE algorithm}

\paragraph{The idea} The policy gradient \eqref{eq:PolicyGradient} is an expectation which can be estimated using Monte Carlo approximation. We obtain the following estimate:
\begin{equation}\label{eq:MCPolicyGrad}
\widehat{\nabla_\theta J}(\pi_\theta) =
\frac{1}{M}\sum_{i=1}^M R(\tau_i)
\sum_{t=0}^{T_i}
\nabla_\theta
\log\pi_\theta(s_t^i, a_t^i)
\end{equation}
This is an unbiased Monte Carlo estimate of the policy gradient. It only requires suitable regularity of the parametric policy model $\theta \longmapsto \pi_\theta$.


\begin{remark}\label{rmk:PolicyGradPseudoLoss} \Cref{eq:MCPolicyGrad} can be used as-is for functions with simple closed-form derivatives.
	In an automatic differentiation framework such as {\normalfont\sffamily PyTorch}, we can instead get the policy gradient from a computational graph with the following pseudo-loss function:
	\begin{equation}\label{eq:PolicyGradPseudoLoss}
	\begin{aligned}
	\tilde{J}(\theta)
	&= \frac{1}{M}\sum_{i=1}^M R(\tau_i)
	\sum_{t=0}^{T_i} \log\pi_\theta(s_t^i, a_t^i)\\
	&=
	\frac{1}{M}\sum_{i=1}^M
	\left(
	\sum_{t=0}^{T_i} \gamma^t r^i_{t}
	\right)
	\sum_{t=0}^{T_i} \log\pi_\theta(s_t^i, a_t^i)
	\end{aligned}
	\end{equation}
\end{remark}

We will simulate multiple trajectories (\textit{episodes}) to perform the gradient update: this is the similar to what is done with batch, mini-batch or stochastic gradient steps. This leads to \cref{algo:REINFORCEbasic}.

\begin{algorithm}[H]
	\caption{Monte Carlo Policy Gradient (REINFORCE)}\label{algo:REINFORCEbasic}
	\KwIn{Arbitrary initial policy $\pi_{\theta_0}$.}
	\KwOut{Optimal parametric policy $\pi_{\theta^*}$.}
	\Repeat{finished}{
		Simulate a trajectory $\tau$\;
		$\boldsymbol{g} \leftarrow (\sum_{t=0}^T \gamma^t r_t)\sum_{t=0}^T \nabla_\theta \log\pi_\theta(s_t, a_t)$\tcp*{policy gradient}
		$\theta \leftarrow \theta + \alpha\boldsymbol{g}$\tcp*{update parameter}
	}
\end{algorithm}

This algorithm can be modified in several ways, by performing the gradient step at every time in the process, or on the entire batch of trajectories. \textcite{Sutton1998} actually use the more fine-grained update scheme wherein the parameter $\theta$ is updated at each step $t$ of every episode\footnote{This allows for fully online learning.}.

\subsection{Variance reduction: temporal structure}


We can re-weigh the log-probability gradients in \cref{eq:PolicyGradient} by exploiting the fact that, for any time $t$, the cumulative rewards $\sum_{t'=0}^{t-1} \gamma^{t'} r_{t'}$ from $0$ to $t-1$ are measurable with respect to the trajectory up to $t$, $\tau_{0:t}$:
\begin{prop}
	The policy gradient \eqref{eq:PolicyGradient} can be rewritten as
	\begin{equation}
	\nabla_\theta J(\pi_\theta) =
	\EE_\pi \left[
	\sum_{t=0}^T
	{\redfont
		\sum_{t'=t}^T \gamma^{t'} r_{t'}
	}
	\nabla_\theta \log\pi_\theta(s_t,a_t)
	\right]
	\end{equation}
	which leads to the policy gradient estimate
	\begin{equation}
	\widehat{\nabla_\theta J}(\pi_\theta) =
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^{T_i}
	{\redfont \gamma^t G^i_t}
	\nabla_\theta \log\pi_\theta (s_t^i, a_t^i)
	\end{equation}
	where\footnote{This quantity is an estimate of the $Q$-function $Q^\pi(s_t, a_t) = \EE [\sum_{t'=t}^T \gamma^{t'-t} r_{t'} \,|\, s_{t}, a_{t} ]$.} $G^i_t = \sum_{t'=t}^T \gamma^{t'-t} r^i_{t'}$.
\end{prop}


\subsection{Variance reduction with baselines}

Given any \textbf{\bluefont baseline} function $b\colon \calS \to \RR$, we can rewrite the policy gradient again as
\begin{equation}\label{eq:PolicyGradBaseline}
\nabla_\theta J(\pi_\theta) =
\EE_\pi \left[
\sum_{t=0}^T
\left(
\sum_{t'=t}^T \gamma^{t'} r_{t'} - {\redfont b(s_t)}
\right)
\nabla_\theta \log\pi_\theta(s_t, a_t)
\right]
\end{equation}
The resulting policy gradient estimate we get is
\begin{equation}\label{eq:MCPolicyGradBaseline}
\widehat{\nabla_\theta J}(\pi_\theta) =
\frac{1}{M}\sum_{i=1}^M
\sum_{t=0}^{T_i}
\gamma^t (G^i_t - {\redfont b(s^i_t)})
\nabla_\theta \log\pi_\theta (s_t^i, a_t^i)
\end{equation}
which is still an unbiased estimate -- but with the added benefit of \textbf{variance reduction} if $b$ is chosen right.

\subsubsection{The best baseline.}

It can be shown that the ``best" baseline in terms of variance reduction is actually the value function:
\[
b^*(s_t) = \EE_{\pi_\theta}\left[\sum_{t'=t}^T \gamma^{t'-t} r_{t'} \scalemid s_t \right]
= V^\pi(s_t)
\]
\ldots which we are already trying to approximate. This suggests that we do some kind of \textbf{bootstrapping}. The approximate policy evaluation ideas from the previous \cref{chap:ApproxRL} will come in handy.

The obvious first idea is nonparametric Monte Carlo estimation:
\[
	\widehat{b}(s_t) = \frac{1}{M}\sum_{i=1}^M G^i_t
\]
This baseline is an unbiased estimate: following \cref{eq:ValueFunctionExpectationLink} the expectation of the $Q$-function estimate $G_t$ under the policy $\pi_\theta$ is
\[
	V^\pi(s_t) =
	\EE_{a_t\sim \pi_\theta(s_t,\cdot)}
	\EE_{\pi_\theta}
	\left[G_t\right] =
	\EE_{a_t\sim\pi_\theta(s_t,\cdot)}
	\EE_{\pi_\theta}
	\left[
	\sum_{t'=t}^T \gamma^{t'-t}r_{t'}
	\scalemid s_t,a_t
	\right]
\]
However, this introduces additional variance and the baseline might become noisy.



\subsection{Parametric Bootstrapping of the baseline}\label{sec:REINFORCEparametricBaseline}

We define the bootstrap estimate $b(s) \coloneqq \widehat{v}_\nu(s)$ lying in a parametric search space $\{\widehat{v}_\nu : \nu \in \mathcal{V}\}$. The value parameter can be iteratively updated using gradient steps alternatively with the policy parameter $\theta$.

For a given trajectory sample $\tau=\{s_0,a_0,r_0,\ldots\}$, introduce the mean-squared error between the returns $G_t = \sum_{t'= t}^T \gamma^{t'-t} r_{t'}$ of the entire trajectory (a nonparametric estimate of the value function) and the output of the value approximator:
\begin{equation}
	\calL(\nu; \tau) =
	\sum_{t=0}^{T} (G_t-\widehat{v}_\nu(s_t))^2
\end{equation}
Then before each update of the policy $\pi_\theta$, update the value parameter $\nu$ using either the gradient of $\calL$.

The adapted episodic learning algorithm with an adaptive baseline is as follows:
\begin{algorithm}
	\caption{REINFORCE with parametric baseline}\label{algo:REINFORCEparamBaseline}
	\Repeat{finished}{
		Simulate a trajectory $\tau$\;
		Compute the returns $(G_t)$ of the trajectory\;
		$\boldsymbol{g} \leftarrow \sum_{t=0}^T \gamma^t (G_t - \widehat{v}_\nu(s_t))\nabla_\theta \log\pi_\theta (s_t,a_t)$ \tcp*{policy gradient}
		$\nu \leftarrow \nu - \beta\nabla_\nu \calL(\nu; \tau)$ \tcp*{update value estimate}
		$\theta \leftarrow \theta + \alpha \boldsymbol{g}$ \tcp*{update policy}
	}
\end{algorithm}

As before (see \cref{rmk:PolicyGradPseudoLoss}), this algorithm can be implemented in an automatic differentiation framework like \textsf{PyTorch} by defining the right computational graph. The associated pseudo-loss would be
\[
\tilde{J}(\theta) = 
\sum_{t=0}^T \gamma^t (G_t - \widehat{v}_\nu(s_t)) \log\pi_\theta(s_t, a_t)
\]

\begin{remark}
	The difference between the returns and the value approximation $G_t - \widehat{v}_\nu(s_t)$ is a (biased) estimate of the temporal-difference error $\delta_t = r_t + \gamma \widehat{v}(s_{t+1}) - \widehat{v}(s_t)$. Thus, the mean-squared error $\calL$ is an estimate of the Bellman error of the value approximator $\widehat{v}_\nu$.
\end{remark}



\section{Bootstrapped returns: Actor-Critic algorithms}


\paragraph{The idea} The enhanced REINFORCE algorithm builds estimates of the value function to compute the policy gradient: this Monte Carlo method is computationally expensive, and may still lead to high variance. To combat this, it might be a good idea to \textit{learn} from the Monte Carlo estimates in a way that gives a consistent estimate that follows the policy gradient updates.

To achieve this, the class of \textbf{\bluefont actor-critic methods} introduces a second search space for approximation of the state(-action) value function and uses \textbf{bootstrapped temporal-difference estimates} for the returns $G_t$, by taking inspiration from TD policy evaluation (see \cref{chap:ApproxRL}). The supervised baseline of \cref{sec:REINFORCEparametricBaseline} was already a first step towards this, but this time we actually use the value estimate for policy evaluation and not only for variance reduction. The subtle difference is further explained in \cite[chap.\ 13.5]{Sutton1998}.

\subsection{Actor-Critic}


Learning the policy is still done by gradient steps, using estimates of the form \cref{eq:MCPolicyGradBaseline}. But this time, we replace in the objective $J$ the Monte Carlo returns $G_t$, which estimate the $Q$-function, by a parametric estimator $\widehat{q}_\omega(s_t, a_t)$ called the \textbf{\bluefont critic}. As the algorithm runs, the critic learns how to value each action suggested by the learned policy $\pi$ (called the \textbf{\bluefont actor}) by looking at the bootstrapped return estimates. The policy gradient becomes
\begin{equation}\label{eq:ActorPolicyGrad}
\widehat{\nabla_\theta J}(\pi_\theta) =
\frac{1}{M}\sum_{i=1}^M
\sum_{t=0}^{T_i} \widehat{q}_\omega(s^i_t,a^i_t)
\nabla_\theta \log\pi_\theta(s^i_t, a^i_t)
\end{equation}

Now, we want the critic $\widehat{q}_\omega$ to minimize the \textbf{Bellman error}
\[
\EE_{\pi_\theta}\left[
(r_t + \gamma\widehat{q}_\omega(s_{t+1}, a_{t+1}) - \widehat{q}_\omega(s_t,a_t))^2
\mid s_t, a_t
\right]
\]
because having it as small as possible will ensure that it follows the dynamic programming principle, making it a ``good" estimate of the real action-state value function $Q^{\pi_\theta}$. The above expectation can be estimated using the $\TD(0)$ error of $\widehat{q}_\omega$:
\[
\delta_t = r_t + \gamma\widehat{q}_\omega(s_{t+1}, a_{t+1}) - \widehat{q}_\omega(s_t,a_t).
\]
which we will seek to minimize for every time step or episode alternatively with the actor loss.

This leads to the following algorithm:

\begin{algorithm}[H]
	\caption{Actor-Critic}\label{algo:BaseActorCritic}
	\KwIn{Initial policy parameter $\theta$, value parameter $\omega$}
	\KwOut{Policy $\pi_{\theta^*}$, action-state value approximation $\widehat{q}_{\omega^*}$}
	\Repeat{finished}{
		Simulate trajectory $\tau$\;
		\ForEach{$t=0,\ldots,T$}{
			$\delta_t \leftarrow r_t + \gamma \widehat{q}_\omega(s_{t+1},a_{t+1}) - \widehat{q}_\omega(s_t,a_t)$ \tcp*{$\TD(0)$ error}
		}
		$\omega \leftarrow \omega + \beta \sum_{t=0}^T\delta_t \nabla_\omega \widehat{q}_\omega (s_t,a_t)$ \tcp*{critic update}
		$\theta \leftarrow \theta + \alpha \sum_{t=0}^T\widehat{q}_\omega(s_t,a_t) \nabla_\theta \log\pi_\theta(s_t, a_t)$ \tcp*{actor update}
	}
\end{algorithm}



\subsection{Actor-Critic with baselines: Advantage Actor-Critic (A2C)}

As we've seen before, a good baseline to introduce in the policy gradient is the value function -- see \cref{eq:PolicyGradBaseline}.
The coefficient before the log-probability gradient in \eqref{eq:PolicyGradient} becomes an estimate of $Q^\pi(s,a) - V^\pi(s) = \EE_{s'}[r(s,a) + \gamma V^\pi(s')\mid s,a]$. We introduce the \textbf{advantage} function
\begin{equation}\label{eq:AdvantageDefn}
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s).
\end{equation}
An unbiased estimate of $A^\pi$ is the temporal difference error of the value function
\[
\delta_t = r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t).
\]
This can be seen by taking the expectation with respect to $s_{t+1}$ conditionally on $(s_t,a_t)$.


Following the spirit of \cref{chap:approxRL} on approximate policy evaluation, this leads to the (bootstrapped) TD error using the parametric approximation $\widehat{v}_\nu$ of the value function:
\begin{equation}
	{\bluefont \delta_t} = r_t + \gamma\widehat{v}_\nu(s_{t+1}) - \widehat{v}_\nu(s_t)
\end{equation}
just as in \cref{chap:approxRL}.
This last step can be seen as a supervised regression step, where we fit the (bootstrapped) returns ${\redfont G_{t}} = r_t + \gamma\widehat{v}_\nu(s_{t+1})$ to the value model predictions $\widehat{v}_\nu(s_t)$, with a mean-squared loss
\begin{equation}
	\calL(\nu) = \sum_{t=0}^T ({\redfont G_{t}} - \widehat{v}_{\nu}(s_t))^2
\end{equation}

Now, we have no need to learn a action-state critic $\widehat{q}_\omega$, since we have TD error estimates that can be expressed only using $\widehat{v}_\nu(s)$ -- which is simpler.

The episodic A2C algorithm is as follows:

\begin{algorithm}[H]
	\caption{Advantage Actor-Critic (A2C), episodic case}\label{algo:AdvantageActorCritic}
	\KwIn{Initial policy parameter $\theta$, value parameter $\nu$}
	\KwOut{Policy $\pi_{\theta^*}$, value approximation $\widehat{v}_{\nu^*}$}
	\Repeat{finished}{
		Simulate trajectory $\tau$\;
		\ForEach{$t=0,\ldots,T$}{
			$\delta_t \leftarrow r_t + \gamma \widehat{v}_\nu(s_{t+1}) - \widehat{v}_\nu(s_t)$ \tcp*{$\TD(0)$ error}
		}
		$\nu \leftarrow \nu + \beta \sum_{t=0}^T\delta_t \nabla_\nu \widehat{v}_\nu (s_t)$ \tcp*{critic update}
		$\theta \leftarrow \theta + \alpha \sum_{t=0}^T \widehat{v}_\nu(s_t) \nabla_\theta \log\pi_\theta(s_t, a_t)$ \tcp*{actor update}
	}
\end{algorithm}


\begin{remark}\label{rmk:A2CAutoDiff}
	To leverage automatic differentiation, appropriate pseudo-losses to define a computational graph for the updates in \cref{algo:AdvantageActorCritic} are
	\begin{subequations}
	\begin{align}
	\tilde{C}(\nu) &= \frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^T\delta^i_t  \widehat{v}_\nu(s^i_t)  \\
	\tilde{J}(\theta) &=
	\frac{1}{M}\sum_{i=1}^M
	\sum_{t=0}^T \widehat{v}_\nu(s^i_t) \log\pi_\theta(s^i_t,a^i_t)
	\end{align}
	\end{subequations}
	where the temporal-difference estimates $\delta^i_t$ must be detached from the graph.

	Equivalently, the pseudo-loss $\tilde{C}(\nu)$ in \cref{rmk:A2CAutoDiff} can be replaced by the MSE $\calL(\nu)$ after detaching $G_t$.
\end{remark}


We create the estimate $G_{t:t+1}$ by only looking at the immediate reward and bootstrapping the rest of the value: this is called \textbf{one-step actor-critic}. It has poor sample efficiency: the return estimates ignore most of the (actual) future rewards and end up bootstrapping too much. However, it naturally allows for \textit{online learning}.	

This can be easily generalized to using more rewards for bootstrapping the returns: \textbf{$\boldsymbol{n}$-step actor-critic} methods look further ahead and use estimates 
\[
	{\redfont G_{t:t+n}}
	= \sum_{k=0}^{n-1}\gamma^k r_{t+k} + \gamma^n \widehat{v}_\nu(s_{t+n})
\]


\paragraph{Batch actor-critic} In batch algorithms, we switch out episodic simulation with epochs of simulation-and-reset batches, where we simulate trajectories (and reset them at terminal states) until a fixed number $B$ of updates have been done.

\begin{algorithm}[H]
\caption{Batch A2C}\label{algo:BatchA2C}
\SetKwFunction{EnvReset}{Reset}
\KwIn{Number of epochs $N$, batch size $B$}
\ForEach{$n=1,\ldots,N$}{
	$s_0 \leftarrow \EnvReset()$\;
	\ForEach{$i=0,\ldots,B-1$}{
		$v_i \leftarrow \widehat{v}_\nu(s_i)$ \tcp*{estimate value}
		Draw action $a_i\sim \pi_\theta$\;
		Get state, reward $(r_i, s_{i+1})$\;
		\tcp{check if we just finished a trajectory}
		\If{$s_{i+1}$ is terminal}{
			$s_{i+1} \leftarrow \EnvReset()$\;
		}
	}
	\eIf{$s_{B}$ is terminal}{
		$v_B \leftarrow 0$\;
	}
	{
		$v_B \leftarrow \widehat{v}_\nu(s_B)$\;
	}
	Compute bootstrapped advantages, returns $(\delta_i, \widehat{G}_i)$\;
	$\nu \leftarrow \nu +\beta\sum_{i=0}^{B-1} \delta_i\nabla_\nu\widehat{v}_\nu(s_i)$\;
	$\theta \leftarrow \theta +\alpha \sum_{i=0}^{B-1} \delta_i \nabla_\theta \log\pi_\theta(s_i, a_i)$\;
}
\end{algorithm}

The returns $\widehat{G}_i$ have to be computed taking into account when trajectories end.
An efficient algorithm is
\[
	\widehat{G}_i =
	\begin{cases}
	r_i + \gamma \widehat{G}_{i+1}& \text{ if } s_i \text{ is not terminal} \\
	r_i & \text{ otherwise}
	\end{cases}
\]
for $i=0,\ldots,B-2$ and $G_{B-1} = r_{B-1} + \gamma v_B$. The $\TD(0)$ errors are $\delta_i = \widehat{G}_i - v_i$.




\end{document}
