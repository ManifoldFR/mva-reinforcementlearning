{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJ_s4_nhhiyo"
   },
   "source": [
    "# Reinforcement Learning with Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.2.0 torchvision pyvirtualdisplay matplotlib seaborn pandas numpy pathlib gym\n",
    "!sudo apt-get install xvfb\n",
    "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
    "!cd mvarl_hands_on && git pull origin master > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from pprint import pprint\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from pathlib import Path\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCsyvh4NhjUc"
   },
   "outputs": [],
   "source": [
    "# The following code is will be used to visualize the environments.\n",
    "\"\"\"\n",
    "def show_video(directory):\n",
    "    html = []\n",
    "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "\"\"\"\n",
    "\n",
    "def show_video(directory):\n",
    "    \"\"\"Using IPython.display.Video\"\"\"\n",
    "    all_vids = list(Path(directory).glob(\"*.mp4\"))\n",
    "    for mp4 in all_vids:\n",
    "        vid = ipythondisplay.Video(mp4)\n",
    "        ipythondisplay.display(vid)\n",
    "    \n",
    "display = Display(visible=0, size=(900, 400))\n",
    "display.start();\n",
    "\n",
    "def make_seed(seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThnSNaSMhnXa"
   },
   "source": [
    "PyTorch is a Python package that provides two high-level features:\n",
    "- Tensor computation (like NumPy) with strong GPU acceleration\n",
    "- Deep neural networks built on a tape-based autograd system\n",
    "\n",
    "At a granular level, PyTorch is a library that consists of the following components:\n",
    "\n",
    "| Component | Description |\n",
    "| ---- | --- |\n",
    "| [**torch**](https://pytorch.org/docs/stable/torch.html) | a Tensor library like NumPy, with strong GPU support |\n",
    "| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n",
    "| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n",
    "| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | a neural networks library deeply integrated with autograd designed for maximum flexibility |\n",
    "| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n",
    "| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n",
    "\n",
    "\n",
    "**Tutorials on PyTorch:** https://pytorch.org/tutorials/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlZQl9TjhuWY"
   },
   "source": [
    "## OpenAI gym\n",
    "We will consider environments provided by OpenAI gym\n",
    "This library provides a large number of environments to test RL algorithm.\n",
    "\n",
    "We will focus on the **CartPole-v1** environment in this lab but we encourage you to also test your code on:\n",
    "* **Acrobot-v1**\n",
    "* **MountainCar-v0**\n",
    "\n",
    "| Env Info          \t| CartPole-v1 \t| Acrobot-v1                \t| MountainCar-v0 \t|\n",
    "|-------------------\t|-------------\t|---------------------------\t|----------------\t|\n",
    "| **Observation Space** \t| Box(4)      \t| Box(6)                    \t| Box(2)         \t|\n",
    "| **Action Space**      \t| Discrete(2) \t| Discrete(3)               \t| Discrete(3)    \t|\n",
    "| **Rewards**           \t| 1 per step  \t| -1 if not terminal else 0 \t| -1 per step    \t|\n",
    "\n",
    "A gym environment is loaded with the command `env = gym.make(env_id)`. Once the environment is created, you need to reset it with `observation = env.reset()` and then you can interact with it using the method step: `observation, reward, done, info = env.step(action)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wfHvW9Hh4H3"
   },
   "outputs": [],
   "source": [
    "# We load CartPole-v1\n",
    "env = gym.make('CartPole-v1')\n",
    "# We wrap it in order to save our experiment on a file.\n",
    "env = Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-fFj5sDiA0C"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"gym-results/openaigym.video.0.16302.video000000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "env.close()\n",
    "show_video(\"./gym-results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd2xLlapilZE"
   },
   "source": [
    "## REINFORCE\n",
    "\n",
    "**Q1: Implement the REINFORCE algorithm**\n",
    "\n",
    "The code is splitted in two parts:\n",
    "* The Model class defines the architecture of our neural network which takes as input the current state and returns the policy,\n",
    "* The Agent class is responsible for the training and evaluation procedure. You will need to code the method `optimize_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwsQ8NSPiCz7"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dim_observation, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.dim_observation = dim_observation\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=self.dim_observation, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=self.n_actions),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action = torch.multinomial(self.forward(state), 1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrgtQMYbiwX7"
   },
   "source": [
    "Create the model based on the properties of the MDP you want to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6ieL_KJirq9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model we created correspond to:\n",
      "Model(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax(dim=0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "env = gym.make(env_id)\n",
    "model = Model(env.observation_space.shape[0], env.action_space.n)\n",
    "print(f'The model we created correspond to:\\n{model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwrVF9kti1e-"
   },
   "source": [
    "We provide a base agent that you will need to extend in the next cell with your implementation of `optimize_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWvKO66ii1zh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = gym.make(config['env_id'])\n",
    "        make_seed(config['seed'])\n",
    "        self.env.seed(config['seed'])\n",
    "        self.model = Model(self.env.observation_space.shape[0],\n",
    "                           self.env.action_space.n)\n",
    "        self.gamma = config['gamma']\n",
    "        \n",
    "        # the optimizer used by PyTorch (Stochastic Gradient, Adagrad, Adam, etc.)\n",
    "        self.optimizer = torch.optim.Adam(self.model.net.parameters(), lr=config['learning_rate'])\n",
    "        self.monitor_env = Monitor(self.env, \"./gym-results\", force=True, \n",
    "                                   video_callable=lambda episode: True)\n",
    "    \n",
    "        self.current_ep = 0\n",
    "        self.rewards = []\n",
    "    \n",
    "    # Method to implement\n",
    "    def _compute_returns(self, rewards):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            The array of rewards of one episode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards at each time step\n",
    "            \n",
    "        Example\n",
    "        -------\n",
    "        for rewards=[1, 2, 3] this method outputs [1 + 2 * gamma + 3 * gamma**2, 2 + 3 * gamma, 3] \n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Method to implement\n",
    "    def optimize_model(self, n_trajectories):\n",
    "        \"\"\"Perform a gradient update using n_trajectories\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expectation card(D) in the formula above\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards of each trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, n_trajectories, n_update):\n",
    "        \"\"\"Training method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expected gradient\n",
    "        n_update : int\n",
    "            The number of gradient updates\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        final_update = self.current_ep + n_update\n",
    "        rewards = self.rewards  # restart the reward record\n",
    "        for episode in range(self.current_ep, final_update):\n",
    "            rewards.append(self.optimize_model(n_trajectories))\n",
    "            print(f'Episode {episode + 1}/{final_update}: rewards ' \n",
    "                  +f'{round(rewards[-1].mean(), 2)} +/- {round(rewards[-1].std(), 2)}')\n",
    "            self.current_ep += 1\n",
    "        \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards[i]) for i in range(len(rewards))))), columns=['Epoch', 'Reward'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "        \n",
    "    def evaluate(self, render=False):\n",
    "        \"\"\"Evaluate the agent on a single trajectory            \n",
    "        \"\"\"\n",
    "        \n",
    "        ## Wrap in torch.no_grad to stop the tape recorder\n",
    "        ## and stop the RAM from blowing up\n",
    "        with torch.no_grad():\n",
    "            observation = self.monitor_env.reset()\n",
    "            observation = torch.from_numpy(observation).float()\n",
    "            reward_episode = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.model.select_action(observation)\n",
    "                observation, reward, done, info = self.monitor_env.step(int(action))\n",
    "                observation = torch.from_numpy(observation).float()\n",
    "                reward_episode = self.gamma * reward_episode + reward\n",
    "        self.monitor_env.close()\n",
    "        if render:\n",
    "            show_video(\"./gym-results\")\n",
    "        print(f'Reward: {reward_episode}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6mhw_A3i_8R"
   },
   "source": [
    "Finally you can implement your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** The REINFORCE algorithm uses an unbiased estimate of the gradient of\n",
    "$$\n",
    "    J(\\pi_\\theta) = \\mathbb E_{\\tau\\sim \\mathbb P_\\theta}\\left[ R(\\tau) \\right]\n",
    "    = \\mathbb E_\\tau \\left[\n",
    "        \\sum_{t=0}^T \\gamma^t r_t\n",
    "    \\right]\n",
    "$$\n",
    "which has exact gradient\n",
    "$$\n",
    "    \\nabla_\\theta J(\\pi_\\theta) = \\mathbb E_{\\tau\\sim\\mathbb P_\\theta}\n",
    "    \\left[ R(\\tau)\\nabla_\\theta \\log \\pi_\\theta \\right] =\n",
    "    \\mathbb E_\\tau\\left[\n",
    "    \\left(\\sum_{t=0}^T \\gamma^t r_t\\right)\n",
    "    \\left(\\sum_{t=0}^T \\nabla_\\theta\\log \\pi_\\theta(s_t, a_t) \\right)\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "We introduce the pseudo-likelihood\n",
    "$$\n",
    "    L(\\theta) = \\frac{1}{M}\\sum_{i=1}^M\n",
    "    \\left(\\sum_{t=0}^T \\gamma^t r^i_t\\right)\n",
    "    \\left(\\sum_{t=0}^T \\log \\pi_\\theta(s^i_t, a^i_t) \\right) \\geq 0\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "    \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "is an unbiased estimate of the policy gradient $\\nabla_\\theta J(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbjP-7WHi9oc"
   },
   "outputs": [],
   "source": [
    "class REINFORCE(BaseAgent):\n",
    "    \n",
    "    def _compute_returns(self, rewards):\n",
    "        num_rew = len(rewards)\n",
    "        exponents = np.arange(num_rew)\n",
    "        gammas = np.power(self.gamma, exponents)\n",
    "        \n",
    "        return rewards.dot(gammas)\n",
    "    \n",
    "        \n",
    "    def optimize_model(self, n_trajectories):\n",
    "\n",
    "        env = self.env\n",
    "        reward_trajectories = np.empty(n_trajectories)\n",
    "        loss = 0.\n",
    "        \n",
    "        for i in range(n_trajectories):\n",
    "            traj_rewards = []  # rewards of the trajectory\n",
    "            traj_proba = 0.  # sum of log-probabilities of trajectory\n",
    "            \n",
    "            # Build trajectory\n",
    "            done = False\n",
    "            obs = env.reset()\n",
    "            obs = torch.from_numpy(obs).float()  # state s0\n",
    "            while not done:\n",
    "                action = self.model.select_action(obs)  # can be cast to int for action idx\n",
    "                # Get proba\n",
    "                prob = self.model(obs)[int(action)]\n",
    "                traj_proba += torch.log(prob)\n",
    "                \n",
    "                \n",
    "                obs, reward, done, info = env.step(int(action))\n",
    "                \n",
    "                obs = torch.from_numpy(obs).float()\n",
    "                # Store the new reward\n",
    "                traj_rewards.append(reward)\n",
    "                \n",
    "                \n",
    "            traj_rewards = np.array(traj_rewards)  # NumPy array\n",
    "            \n",
    "            # Get total reward\n",
    "            total_reward = self._compute_returns(traj_rewards)  # NumPy array\n",
    "            reward_trajectories[i] = total_reward\n",
    "            \n",
    "            loss = loss + total_reward * traj_proba / n_trajectories  # accumulate the negative criterion\n",
    "        \n",
    "        env.close()  # important\n",
    "        \n",
    "        loss = -loss\n",
    "        \n",
    "        # The following lines take care of the gradient descent step for the variable loss\n",
    "        # that you need to compute.\n",
    "        print(\"Loss:\", loss.data.numpy())\n",
    "        \n",
    "        # Discard previous gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Compute the gradient \n",
    "        loss.backward()\n",
    "        # Do the gradient descent step\n",
    "        self.optimizer.step()\n",
    "        return reward_trajectories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZAo0K8VjCYj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config is:\n",
      "{'env_id': 'CartPole-v1', 'gamma': 1.0, 'learning_rate': 0.015, 'seed': 1235}\n"
     ]
    }
   ],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "learning_rate = 0.015\n",
    "gamma = 1.0  # every second counts the same\n",
    "seed = 1235\n",
    "\n",
    "config = {\n",
    "    'env_id': env_id,\n",
    "    'learning_rate': learning_rate,\n",
    "    'seed': seed,\n",
    "    'gamma': gamma\n",
    "}\n",
    "\n",
    "print(\"Current config is:\")\n",
    "pprint(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4Ag7E3qjJas"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 315.27423\n",
      "Episode 1/64: rewards 19.12 +/- 9.49\n",
      "Loss: 521.7329\n",
      "Episode 2/64: rewards 24.73 +/- 11.84\n",
      "Loss: 386.60022\n",
      "Episode 3/64: rewards 21.64 +/- 9.51\n",
      "Loss: 510.77228\n",
      "Episode 4/64: rewards 23.37 +/- 13.92\n",
      "Loss: 580.80133\n",
      "Episode 5/64: rewards 24.76 +/- 15.26\n",
      "Loss: 520.22894\n",
      "Episode 6/64: rewards 24.04 +/- 13.38\n",
      "Loss: 558.75836\n",
      "Episode 7/64: rewards 25.52 +/- 12.8\n",
      "Loss: 622.01666\n",
      "Episode 8/64: rewards 26.6 +/- 14.45\n",
      "Loss: 740.3721\n",
      "Episode 9/64: rewards 28.71 +/- 16.4\n",
      "Loss: 964.107\n",
      "Episode 10/64: rewards 32.19 +/- 19.85\n",
      "Loss: 1108.002\n",
      "Episode 11/64: rewards 32.95 +/- 23.93\n",
      "Loss: 1111.6567\n",
      "Episode 12/64: rewards 34.43 +/- 21.95\n",
      "Loss: 908.49414\n",
      "Episode 13/64: rewards 32.32 +/- 17.95\n",
      "Loss: 1045.6062\n",
      "Episode 14/64: rewards 35.4 +/- 18.0\n",
      "Loss: 1198.4823\n",
      "Episode 15/64: rewards 37.68 +/- 20.54\n",
      "Loss: 1046.5044\n",
      "Episode 16/64: rewards 35.75 +/- 18.17\n",
      "Loss: 1075.5834\n",
      "Episode 17/64: rewards 36.16 +/- 18.48\n",
      "Loss: 1223.8119\n",
      "Episode 18/64: rewards 38.48 +/- 20.45\n",
      "Loss: 1658.1143\n",
      "Episode 19/64: rewards 43.35 +/- 26.54\n",
      "Loss: 1474.7821\n",
      "Episode 20/64: rewards 42.77 +/- 22.29\n",
      "Loss: 1296.2067\n",
      "Episode 21/64: rewards 41.8 +/- 17.36\n",
      "Loss: 1466.902\n",
      "Episode 22/64: rewards 42.68 +/- 22.25\n",
      "Loss: 1585.6609\n",
      "Episode 23/64: rewards 45.37 +/- 22.09\n",
      "Loss: 2203.3499\n",
      "Episode 24/64: rewards 53.69 +/- 26.25\n",
      "Loss: 2504.766\n",
      "Episode 25/64: rewards 55.01 +/- 32.45\n",
      "Loss: 2836.0923\n",
      "Episode 26/64: rewards 61.56 +/- 29.44\n",
      "Loss: 3081.7432\n",
      "Episode 27/64: rewards 62.01 +/- 35.74\n",
      "Loss: 5043.2134\n",
      "Episode 28/64: rewards 79.73 +/- 47.23\n",
      "Loss: 3511.9473\n",
      "Episode 29/64: rewards 66.47 +/- 39.52\n",
      "Loss: 3380.1147\n",
      "Episode 30/64: rewards 69.31 +/- 32.61\n",
      "Loss: 4506.636\n",
      "Episode 31/64: rewards 79.59 +/- 40.41\n",
      "Loss: 6070.8706\n",
      "Episode 32/64: rewards 92.68 +/- 46.72\n",
      "Loss: 7249.139\n",
      "Episode 33/64: rewards 104.37 +/- 46.05\n",
      "Loss: 7793.4917\n",
      "Episode 34/64: rewards 112.09 +/- 38.79\n",
      "Loss: 7610.3677\n",
      "Episode 35/64: rewards 110.93 +/- 40.93\n",
      "Loss: 8999.748\n",
      "Episode 36/64: rewards 123.97 +/- 35.54\n",
      "Loss: 10140.078\n",
      "Episode 37/64: rewards 132.51 +/- 38.22\n",
      "Loss: 9560.032\n",
      "Episode 38/64: rewards 129.53 +/- 37.4\n",
      "Loss: 11307.697\n",
      "Episode 39/64: rewards 140.39 +/- 46.09\n",
      "Loss: 11550.833\n",
      "Episode 40/64: rewards 143.15 +/- 45.39\n",
      "Loss: 11215.28\n",
      "Episode 41/64: rewards 139.75 +/- 51.63\n",
      "Loss: 10564.065\n",
      "Episode 42/64: rewards 140.83 +/- 36.97\n",
      "Loss: 10986.353\n",
      "Episode 43/64: rewards 143.57 +/- 39.73\n",
      "Loss: 11400.541\n",
      "Episode 44/64: rewards 148.44 +/- 38.97\n",
      "Loss: 15552.787\n",
      "Episode 45/64: rewards 175.03 +/- 47.7\n",
      "Loss: 18317.928\n",
      "Episode 46/64: rewards 187.67 +/- 62.78\n",
      "Loss: 20243.656\n",
      "Episode 47/64: rewards 199.76 +/- 68.18\n",
      "Loss: 18813.236\n",
      "Episode 48/64: rewards 193.96 +/- 64.07\n",
      "Loss: 20091.488\n",
      "Episode 49/64: rewards 206.13 +/- 56.31\n",
      "Loss: 21228.467\n",
      "Episode 50/64: rewards 211.04 +/- 61.07\n",
      "Loss: 26279.984\n",
      "Episode 51/64: rewards 229.6 +/- 82.06\n",
      "Loss: 22702.516\n",
      "Episode 52/64: rewards 220.37 +/- 57.05\n",
      "Loss: 23517.572\n",
      "Episode 53/64: rewards 221.63 +/- 64.69\n",
      "Loss: 23319.107\n",
      "Episode 54/64: rewards 219.77 +/- 70.45\n",
      "Loss: 26258.854\n",
      "Episode 55/64: rewards 235.28 +/- 67.82\n",
      "Loss: 30437.758\n",
      "Episode 56/64: rewards 254.31 +/- 76.93\n",
      "Loss: 40093.61\n",
      "Episode 57/64: rewards 295.2 +/- 80.66\n",
      "Loss: 45095.59\n",
      "Episode 58/64: rewards 300.99 +/- 120.0\n",
      "Loss: 56874.44\n",
      "Episode 59/64: rewards 340.13 +/- 127.38\n",
      "Loss: 63817.367\n",
      "Episode 60/64: rewards 362.93 +/- 131.33\n",
      "Loss: 79435.46\n",
      "Episode 61/64: rewards 413.24 +/- 120.78\n",
      "Loss: 90507.73\n",
      "Episode 62/64: rewards 448.97 +/- 85.42\n",
      "Loss: 92207.65\n",
      "Episode 63/64: rewards 454.45 +/- 73.56\n",
      "Loss: 108267.02\n",
      "Episode 64/64: rewards 498.25 +/- 15.03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = REINFORCE(config)\n",
    "\n",
    "agent.train(n_trajectories=75, n_update=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3_dvBl8jUc3"
   },
   "source": [
    "Now, we evaluate the agent over multiple episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPyqbIiCjO_-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"gym-results/openaigym.video.1.24149.video000000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "agent.evaluate(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Save the model\n",
    "datetag = datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "\n",
    "PATH = f\"saved_models/reinforce_{datetag}.pth\"\n",
    "PATH\n",
    "\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHM2_CRFkNoA"
   },
   "source": [
    "## Policy Evaluation as Supervised Learning\n",
    "\n",
    "**Q2: Implement batched gradient algorithm**\n",
    "\n",
    "Define network for value function (ValueNetwork) and policy (ActorNetwork)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ValueNetwork` is an estimate $v_\\nu$ (parameterized by $\\nu \\in \\mathcal V$) of the value function $V^{\\pi_\\theta}$ of the current policy model $\\pi_\\theta$ (parameterized by $\\theta\\in\\Theta$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GKxKzs5khxM"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    Approximation of the value function V under the current policy.\n",
    "    \n",
    "    This is the 'critic'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self(x).detach().numpy()[0]\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    Policy model network for the agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.softmax(self.fc3(out), dim=-1)\n",
    "        return out\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        return torch.multinomial(self(x), 1).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBpNVkaAKLQz"
   },
   "source": [
    "The following `EvalAgent` defines an **evaluation agent**: using the prescribed policy $\\pi$, it runs trajectories to fit an estimate of the value function $V^\\pi$ modelled as a neural network $\\hat{v}_\\nu(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQ4Wu5B5KIIO"
   },
   "outputs": [],
   "source": [
    "class EvalAgent:\n",
    "    \"\"\"\n",
    "    This agent runs the prescribed policy (but does not learn it)\n",
    "    \"\"\"\n",
    "    def __init__(self, config, policy):\n",
    "        self.config = config\n",
    "        self.env = gym.make(config['env_id'])\n",
    "        make_seed(config['seed'])\n",
    "        self.env.seed(config['seed'])\n",
    "        self.monitor_env = Monitor(self.env, \"./gym-results\",\n",
    "                                   force=True, video_callable=lambda episode: True)\n",
    "        self.gamma = config['gamma']\n",
    "        self.policy: ActorNetwork = policy\n",
    "\n",
    "        self.value_hidden = 16\n",
    "        # Our network\n",
    "        self.value_network: ValueNetwork = ValueNetwork(\n",
    "            self.env.observation_space.shape[0], self.value_hidden, 1)\n",
    "\n",
    "        # optimizer -- singular: only for the value network\n",
    "        self.value_network_optimizer = optim.RMSprop(\n",
    "            self.value_network.parameters(),\n",
    "            lr=config['value_network']['learning_rate'])\n",
    "    \n",
    "    def _compute_returns(self, rewards, dones, next_value):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step.\n",
    "        We are going to fit the value_network on this.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            An array of shape (batch_size,) containing the rewards given by the env\n",
    "        dones : array\n",
    "            An array of shape (batch_size,) containing the done bool indicator given by the env\n",
    "        next_value : float\n",
    "            The (estimated) value of the next state given by the value network\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        returns : array\n",
    "            The estimate of the advantage -- or rather the returns\n",
    "        \"\"\"\n",
    "        output = np.empty_like(rewards)\n",
    "        done_idx = np.argwhere(dones).ravel()  # find idx in batch where trajectory terminates\n",
    "        num_dones = dones.sum()\n",
    "        \n",
    "        for i in range(len(done_idx)):\n",
    "            t_final = done_idx[i]\n",
    "            if i > 0:\n",
    "                t_start = done_idx[i-1] + 1\n",
    "            else:\n",
    "                t_start = 0\n",
    "            output[t_final] = rewards[t_final]  # final reward\n",
    "            for t in range(1, t_final-t_start+1):  # use Bellman equation to compute all returns\n",
    "                output[t_final-t] = rewards[t_final-t] + self.gamma * output[t_final-t+1]\n",
    "        \n",
    "        \n",
    "        if num_dones == 0 or not(dones[-1]):\n",
    "            if num_dones > 0:\n",
    "                t_start = done_idx[-1] + 1\n",
    "            else:\n",
    "                t_start = 0\n",
    "            t_final = len(rewards) - 1\n",
    "            if t_start <= t_final:\n",
    "                output[t_final] = rewards[t_final] + self.gamma * next_value\n",
    "                for t in range(1, t_final-t_start+1):  # use Bellman equation to compute all returns\n",
    "                    output[t_final-t] = rewards[t_final-t] + self.gamma * output[t_final-t+1]\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def optimize_model(self, observations, returns): #actions, returns, advantages): what ?\n",
    "        \"\"\"Perform a gradient update using provided transitions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations : array\n",
    "            The observations\n",
    "        returns : array\n",
    "            The returns from each state\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss_value: int  <--- what ??\n",
    "            The loss value\n",
    "        \"\"\"\n",
    "        # Cast our array buffers into tensors\n",
    "        returns = torch.from_numpy(returns[:, None]).float()  # shape (batch_size,1)\n",
    "        observations = torch.from_numpy(observations).float()\n",
    "        \n",
    "        \n",
    "        ## STEP ONE: FIT THE VALUE NETWORK TO THE RETURNS\n",
    "        network_values = self.value_network(observations)  # shape (batch_size,)\n",
    "        mse_values = F.mse_loss(network_values, returns)\n",
    "        #print(\"MSE values:\", mse_values.item())\n",
    "        \n",
    "        # rmk: graph of mse_values only depends on weights of value_network\n",
    "        mse_values.backward()  # backprop step\n",
    "        self.value_network_optimizer.step()  # optimizer step\n",
    "        self.value_network_optimizer.zero_grad()  # zero the gradients\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def training_batch(self, epochs, batch_size):\n",
    "        \"\"\"Train the model over multiple epochs, by batches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : int\n",
    "            Number of epochs\n",
    "        batch_size : int\n",
    "            The size of a batch\n",
    "        \"\"\"\n",
    "        episode_count = 0\n",
    "        actions = np.empty((batch_size,), dtype=np.int)\n",
    "        dones   = np.empty((batch_size,), dtype=np.bool)\n",
    "        rewards = np.empty((batch_size,), dtype=np.float)\n",
    "        \n",
    "        ## shape (batch_size, *obs_dims) -- buffer for observations\n",
    "        observations = np.empty(\n",
    "            (batch_size,) + self.env.observation_space.shape,\n",
    "            dtype=np.float)\n",
    "        \n",
    "        env = self.env\n",
    "        \n",
    "        obs = env.reset()\n",
    "        mse_test = []\n",
    "\n",
    "        ## EPISODE = TRAJECTORY\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Collect the variables computed at the previous step\n",
    "            for i in range(batch_size):\n",
    "                observations[i] = obs  # just observed s_t\n",
    "                obs_ = torch.from_numpy(obs).float()  # tensor\n",
    "                action = self.policy.select_action(obs_)  # act on just observed, action a_t\n",
    "                actions[i] = int(action)\n",
    "                \n",
    "                obs, reward, done, _ = self.env.step(int(action))  # step with env\n",
    "                \n",
    "                dones[i] = done\n",
    "                rewards[i] = reward\n",
    "                \n",
    "                # check if the observation we just got was terminal\n",
    "                if dones[i]:\n",
    "                    obs = self.env.reset()\n",
    "\n",
    "            # If our episode didn't end on the last step we need to compute the value for the last state\n",
    "            if dones[-1]:\n",
    "                next_value = 0  # no value function adjustment\n",
    "            else:\n",
    "                obs_ = torch.from_numpy(obs).float()\n",
    "                next_value = self.value_network(obs_)\n",
    "            \n",
    "            # Update episode_count\n",
    "            episode_count += sum(dones)\n",
    "\n",
    "            # Compute returns over the batch\n",
    "            returns = self._compute_returns(rewards, dones, next_value)\n",
    "            \n",
    "            # Learning step !\n",
    "            self.optimize_model(observations, returns)\n",
    "            \n",
    "            obs = env.reset()\n",
    "            \n",
    "            # Run a test of the value fit, every 50 epochs\n",
    "            if (epoch > 0 and epoch % 25 == 0) or epoch == epochs - 1:\n",
    "                L = []\n",
    "                for _ in range(10):\n",
    "                    obs_states, y_mc = self.evaluate()\n",
    "                    obs_states_tensor = torch.from_numpy(obs_states).float()  # no-copy tensor\n",
    "                    y_hat = self.value_network(obs_states_tensor).detach().numpy()\n",
    "                    err = y_mc - y_hat\n",
    "                    mse = np.mean(err ** 2)\n",
    "                    L.append(mse.item())\n",
    "                mse_test.append(L)\n",
    "                print(f'Epoch {epoch}/{epochs}: Value MSE: {np.mean(mse)}')\n",
    "\n",
    "        env.close()\n",
    "        \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], mse_test[i]) for i in range(len(mse_test))))), columns=['Epoch', 'MSE'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"MSE\", data=r, ci='sd');\n",
    "\n",
    "        print(f'The training was done over a total of {episode_count} episodes')\n",
    "\n",
    "    def evaluate(self, render=False):\n",
    "        \"\"\"Returns the observations and the estimated V-function (using first visit Monte-Carlo)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states : array\n",
    "            Observations\n",
    "        returns : array\n",
    "            The estimate value function of each state\n",
    "        \"\"\"\n",
    "        env = self.monitor_env if render else self.env\n",
    "        observation = env.reset()\n",
    "        states = [observation.copy()]\n",
    "        rewards= []\n",
    "        observation = torch.from_numpy(observation).float()\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        with torch.no_grad():  # avoid wrecking the RAM\n",
    "            while not done:\n",
    "                action = self.policy.select_action(observation)\n",
    "\n",
    "\n",
    "                observation, reward, done, info = env.step(int(action))\n",
    "                for i in range(steps):\n",
    "                    rewards[i] = rewards[i] + np.power(self.gamma, steps-i)*reward\n",
    "                rewards.append(reward)\n",
    "                if not done:\n",
    "                    states.append(observation.copy())\n",
    "                observation = torch.from_numpy(observation).float()\n",
    "                steps += 1\n",
    "\n",
    "        env.close()\n",
    "        if render:\n",
    "            show_video(\"./gym-results\")\n",
    "            print(f'Reward: {reward_episode}')\n",
    "        states = np.array(states).reshape(-1, self.env.observation_space.shape[0])\n",
    "        returns = np.array(rewards).reshape(-1,1)\n",
    "        return states, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IY1VIUHLNKn3"
   },
   "source": [
    "Define configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fvc-7jXDNNkG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config_td is:\n",
      "{'env_id': 'CartPole-v1',\n",
      " 'gamma': 0.99,\n",
      " 'seed': 1,\n",
      " 'value_network': {'learning_rate': 0.0018, 'reference': './CartPole_value.pt'}}\n"
     ]
    }
   ],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "value_learning_rate = 0.0018\n",
    "gamma = 0.99\n",
    "seed = 1\n",
    "\n",
    "config_td = {\n",
    "    'env_id': env_id,\n",
    "    'gamma': gamma,\n",
    "    'seed': seed,\n",
    "    'value_network': {'learning_rate': value_learning_rate, 'reference': './CartPole_value.pt'}\n",
    "}\n",
    "\n",
    "print(\"Current config_td is:\")\n",
    "pprint(config_td)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2lRnaF3NPyy"
   },
   "source": [
    "We create the policy network: for now we will use a prescribed, pre-trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkbHK_lXNShg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi(state) =  [1]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(config_td['env_id'])\n",
    "\n",
    "## Define the policy network\n",
    "policy = ActorNetwork(env.observation_space.shape[0], 16, env.action_space.n)  # policy model network\n",
    "policy.load_state_dict(torch.load('./mvarl_hands_on/data/CartPole_actor.pt'))  # load pre-trained weights\n",
    "state = torch.from_numpy(env.reset()).float()  # initial state\n",
    "print(\"pi(state) = \", policy.select_action(state))\n",
    "# del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y51M4EgtNU7Z"
   },
   "source": [
    "Run agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PFvnYLnNGvX"
   },
   "outputs": [],
   "source": [
    "agent = EvalAgent(config=config_td, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PFvnYLnNGvX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "dones:   [False False False False False False False False False False False  True\n",
      " False False False False False False False False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11.36151283, 10.46617457,  9.5617925 ,  8.64827525,  7.72553056,\n",
       "        6.79346521,  5.85198506,  4.90099501,  3.940399  ,  2.9701    ,\n",
       "        1.99      ,  1.        ,  7.72553056,  6.79346521,  5.85198506,\n",
       "        4.90099501,  3.940399  ,  2.9701    ,  1.99      ,  1.        ])"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For sanity\n",
    "def _test_compute_returns():\n",
    "    n = 20\n",
    "    rewards = np.ones(n)\n",
    "    dones = np.zeros(n, dtype=np.bool)\n",
    "    dones[11] = True\n",
    "    print(\"rewards:\", rewards)\n",
    "    print(\"dones:  \", dones)\n",
    "    \n",
    "    next_value = 0\n",
    "    res = agent._compute_returns(rewards, dones, next_value)\n",
    "    return res\n",
    "\n",
    "_test_compute_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PFvnYLnNGvX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/800: Value MSE: 6699.278353901591\n",
      "Epoch 50/800: Value MSE: 5991.503648337278\n",
      "Epoch 75/800: Value MSE: 5000.364631369248\n",
      "Epoch 100/800: Value MSE: 3896.606354796555\n",
      "Epoch 125/800: Value MSE: 2774.2767504865096\n",
      "Epoch 150/800: Value MSE: 1984.9110025409723\n",
      "Epoch 175/800: Value MSE: 1343.6022859876177\n",
      "Epoch 200/800: Value MSE: 867.6867489232842\n",
      "Epoch 225/800: Value MSE: 774.4282754360681\n",
      "Epoch 250/800: Value MSE: 625.193216949315\n",
      "Epoch 275/800: Value MSE: 611.1461388182929\n",
      "Epoch 300/800: Value MSE: 678.7646788746032\n",
      "Epoch 325/800: Value MSE: 632.5248268384606\n",
      "Epoch 350/800: Value MSE: 685.1032253202782\n",
      "Epoch 375/800: Value MSE: 672.394533104876\n",
      "Epoch 400/800: Value MSE: 581.2607424291173\n",
      "Epoch 425/800: Value MSE: 615.2472864675782\n",
      "Epoch 450/800: Value MSE: 670.8317121083143\n",
      "Epoch 475/800: Value MSE: 611.3587642770331\n",
      "Epoch 500/800: Value MSE: 1065.010844490887\n",
      "Epoch 525/800: Value MSE: 610.2115462206018\n",
      "Epoch 550/800: Value MSE: 614.0521022066032\n",
      "Epoch 575/800: Value MSE: 592.7664596623254\n",
      "Epoch 600/800: Value MSE: 586.6313623191586\n",
      "Epoch 625/800: Value MSE: 619.1607027610471\n",
      "Epoch 650/800: Value MSE: 609.0074146571092\n",
      "Epoch 675/800: Value MSE: 590.8102426893481\n",
      "Epoch 700/800: Value MSE: 614.1035945173315\n",
      "Epoch 725/800: Value MSE: 596.6747355701127\n",
      "Epoch 750/800: Value MSE: 601.3047584759796\n",
      "Epoch 775/800: Value MSE: 949.6427164313317\n",
      "Epoch 799/800: Value MSE: 613.7440861025387\n",
      "The training was done over a total of 834 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 800\n",
    "batch_size = 512\n",
    "agent.training_batch(epochs=EPOCHS, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor Critic (A2C)\n",
    "**Q3: Implement the A2C method**\n",
    "\n",
    "As usual we provide a structure you can use as starting point.\n",
    "\n",
    "\n",
    "\n",
    "**Note:** try to reuse previous parts of previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4EmYpAsjhKh"
   },
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = gym.make(config['env_id'])\n",
    "        make_seed(config['seed'])\n",
    "        self.env.seed(config['seed'])\n",
    "        self.monitor_env = Monitor(self.env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
    "        self.gamma = config['gamma']\n",
    "        self.entropy = config['entropy']\n",
    "        \n",
    "        # Our two networks\n",
    "        self.value_network = ValueNetwork(self.env.observation_space.shape[0], 16, 1)\n",
    "        self.actor_network = ActorNetwork(self.env.observation_space.shape[0], 16, self.env.action_space.n)\n",
    "        \n",
    "        # Their optimizers\n",
    "        self.value_network_optimizer: optim.Optimizer = optim.RMSprop(\n",
    "            self.value_network.parameters(), lr=config['value_network']['learning_rate'])\n",
    "        self.actor_network_optimizer: optim.Optimizer = optim.RMSprop(\n",
    "            self.actor_network.parameters(), lr=config['actor_network']['learning_rate'])\n",
    "        \n",
    "    # Hint: use it during training_batch\n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            An array of shape (batch_size,) containing the rewards given by the env\n",
    "        dones : array\n",
    "            An array of shape (batch_size,) containing the done bool indicator given by the env\n",
    "        values : array\n",
    "            An array of shape (batch_size,) containing the values given by the value network\n",
    "        next_value : float\n",
    "            The value of the next state given by the value network\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        returns : array\n",
    "            The cumulative discounted rewards\n",
    "        advantages : array\n",
    "            The advantages\n",
    "        \"\"\"\n",
    "        output = np.empty_like(rewards)\n",
    "        done_idx = np.argwhere(dones).ravel()  # find idx in batch where trajectory terminates\n",
    "        num_dones = dones.sum()\n",
    "        \n",
    "        for i in range(len(done_idx)):\n",
    "            t_final = done_idx[i]\n",
    "            if i > 0:\n",
    "                t_start = done_idx[i-1] + 1\n",
    "            else:\n",
    "                t_start = 0\n",
    "            output[t_final] = rewards[t_final]  # final reward\n",
    "            for t in range(1, t_final-t_start+1):  # use Bellman equation to compute all returns\n",
    "                output[t_final-t] = rewards[t_final-t] + self.gamma * output[t_final-t+1]\n",
    "        \n",
    "        \n",
    "        if num_dones == 0 or not(dones[-1]):\n",
    "            if num_dones > 0:\n",
    "                t_start = done_idx[-1] + 1\n",
    "            else:\n",
    "                t_start = 0\n",
    "            t_final = len(rewards) - 1\n",
    "            if t_start <= t_final:\n",
    "                output[t_final] = rewards[t_final] + self.gamma * next_value\n",
    "                for t in range(1, t_final-t_start+1):  # use Bellman equation to compute all returns\n",
    "                    output[t_final-t] = rewards[t_final-t] + self.gamma * output[t_final-t+1]\n",
    "        \n",
    "        \"\"\"advantages = rewards \n",
    "        advantages[:-1] += self.gamma * values[1:] - values[:-1]\n",
    "        advantages[-1] += self.gamma * next_value - values[-1]\"\"\"\n",
    "        advantages = output - values\n",
    "        \n",
    "        return output, advantages\n",
    "\n",
    "    def training_batch(self, epochs, batch_size):\n",
    "        \"\"\"Perform a training by batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : int\n",
    "            Number of epochs\n",
    "        batch_size : int\n",
    "            The size of a batch\n",
    "        \"\"\"\n",
    "        env = self.env\n",
    "        episode_count = 0\n",
    "        actions = np.empty((batch_size,), dtype=np.int)\n",
    "        dones = np.empty((batch_size,), dtype=np.bool)\n",
    "        rewards, values = np.empty((2, batch_size), dtype=np.float)\n",
    "        observations = np.empty((batch_size,) + self.env.observation_space.shape, dtype=np.float)\n",
    "        obs = self.env.reset()\n",
    "        rewards_test = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Lets collect one batch\n",
    "            for i in range(batch_size):\n",
    "                observations[i] = obs  # just observed s_t\n",
    "                obs_ = torch.from_numpy(obs).float()  # tensor\n",
    "                action = self.actor_network.select_action(obs_)  # act on just observed, action a_t\n",
    "                actions[i] = int(action)\n",
    "                \n",
    "                ## Run a step\n",
    "                obs, reward, done, _ = self.env.step(int(action))  # step with env\n",
    "                \n",
    "                values[i] = self.value_network(torch.from_numpy(obs).float())\n",
    "                \n",
    "                # Store status & reward\n",
    "                dones[i]   = done\n",
    "                rewards[i] = reward\n",
    "                \n",
    "                # check if the observation we just got was terminal\n",
    "                if dones[i]:\n",
    "                    obs = env.reset()\n",
    "\n",
    "            # If our episode didn't end on the last step we need to compute the value for the last state\n",
    "            if dones[-1]:\n",
    "                next_value = 0  # no value function adjustment\n",
    "            else:\n",
    "                obs_ = torch.from_numpy(obs).float()\n",
    "                next_value = self.value_network.predict(obs_)\n",
    "            \n",
    "            \n",
    "            # Update episode_count\n",
    "            episode_count += sum(dones)\n",
    "\n",
    "            # Compute returns and advantages\n",
    "            returns, advantages = self._returns_advantages(rewards, dones, values, next_value)\n",
    "\n",
    "            # Learning step !\n",
    "            self.optimize_model(observations, actions, returns, dones, advantages)\n",
    "            \n",
    "            \n",
    "            # Test it every 50 epochs\n",
    "            if epoch == 10 or (epoch > 0 and epoch % 25 == 0) or epoch == epochs - 1:\n",
    "                rewards_test.append(np.array([self.evaluate() for _ in range(50)]))\n",
    "                print(f'Epoch {epoch}/{epochs}: Mean rewards: {round(rewards_test[-1].mean(), 2)}, Std: {round(rewards_test[-1].std(), 2)}')\n",
    "\n",
    "                # Early stopping\n",
    "                if rewards_test[-1].mean() > 490 and epoch != epochs -1:\n",
    "                    print('Early stopping !')\n",
    "                    break\n",
    "            \n",
    "            obs = self.env.reset()\n",
    "        \n",
    "        env.close()\n",
    "        \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards_test[i]) for i in range(len(rewards_test))))), columns=['Epoch', 'Reward'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "        \n",
    "        print(f'The training was done over a total of {episode_count} episodes')\n",
    "\n",
    "    def optimize_model(self, observations, actions, returns, dones, advantages):\n",
    "        n_trajs = np.sum(dones)\n",
    "        actions_idx = actions\n",
    "        actions      = F.one_hot(torch.from_numpy(actions), self.env.action_space.n)\n",
    "        returns      = torch.from_numpy(returns[:, None]).float()\n",
    "        advantages   = torch.from_numpy(advantages).float()\n",
    "        observations = torch.from_numpy(observations).float()  # shape (batch_size,)\n",
    "        \n",
    "        self.value_network_optimizer.zero_grad()\n",
    "        self.actor_network_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # MSE loss\n",
    "        net_values: torch.Tensor = self.value_network(observations)\n",
    "        mse_loss = F.mse_loss(net_values, returns)\n",
    "        \n",
    "        mse_loss.backward()\n",
    "        self.value_network_optimizer.step()\n",
    "        \n",
    "        # Actor & Entropy loss\n",
    "        \n",
    "        prob: torch.Tensor = self.actor_network(observations)  # shape (batch_size,action_space)\n",
    "        prob_act = prob[:, actions_idx]  # size (batch_size,)\n",
    "        \n",
    "        if not dones[-1]:\n",
    "            n_trajs += 1\n",
    "        \n",
    "        reg_terms = torch.sum(prob * prob.log(), dim=1)  # entropic regularization term\n",
    "        loss: torch.Tensor = -torch.sum(prob_act.log()*advantages)/n_trajs#+reg_terms.sum()\n",
    "        print(\"Actor Loss:\", loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        self.actor_network_optimizer.step()\n",
    "        \n",
    "        return loss.detach().numpy()\n",
    "\n",
    "    def evaluate(self, render=False):\n",
    "        env = self.monitor_env if render else self.env\n",
    "        observation = env.reset()\n",
    "        observation = torch.from_numpy(observation).float()\n",
    "        reward_episode = 0\n",
    "        done = False\n",
    "        with torch.no_grad():\n",
    "            while not done:\n",
    "                policy = self.actor_network(observation)\n",
    "                action = torch.multinomial(policy, 1)  # draw an action\n",
    "                observation, reward, done, info = env.step(int(action))\n",
    "                observation = torch.from_numpy(observation).float()\n",
    "                reward_episode += reward\n",
    "            \n",
    "        env.close()\n",
    "        if render:\n",
    "            show_video(\"./gym-results\")\n",
    "            print(f'Reward: {reward_episode}')\n",
    "        return reward_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sEwrInsjkDH"
   },
   "source": [
    "Create configuration for A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwDDx78wjmG5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config_a2c is:\n",
      "{'actor_network': {'learning_rate': 0.0002},\n",
      " 'entropy': 0.001,\n",
      " 'env_id': 'CartPole-v1',\n",
      " 'gamma': 0.99,\n",
      " 'seed': 1,\n",
      " 'value_network': {'learning_rate': 0.0002}}\n"
     ]
    }
   ],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "value_learning_rate = 0.0002\n",
    "actor_learning_rate = 0.0002\n",
    "gamma = 0.99\n",
    "entropy = 0.001\n",
    "seed = 1\n",
    "\n",
    "config_a2c = {\n",
    "    'env_id': env_id,\n",
    "    'gamma': gamma,\n",
    "    'seed': seed,\n",
    "    'value_network': {'learning_rate': value_learning_rate},\n",
    "    'actor_network': {'learning_rate': actor_learning_rate},\n",
    "    'entropy': entropy\n",
    "}\n",
    "\n",
    "print(\"Current config_a2c is:\")\n",
    "pprint(config_a2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xW7fe-8jvzY"
   },
   "source": [
    "Run the learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrZRJ7-yjryp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Loss: 35654.60546875\n",
      "Actor Loss: 40547.9296875\n",
      "Actor Loss: 45819.84375\n",
      "Actor Loss: 58274.421875\n",
      "Actor Loss: 43025.3125\n",
      "Actor Loss: 60653.46484375\n",
      "Actor Loss: 64537.1875\n",
      "Actor Loss: 35183.2578125\n",
      "Actor Loss: 78276.90625\n",
      "Actor Loss: 40241.43359375\n",
      "Actor Loss: 67549.890625\n",
      "Epoch 10/300: Mean rewards: 20.58, Std: 11.87\n",
      "Actor Loss: 36488.6015625\n",
      "Actor Loss: 36724.83984375\n",
      "Actor Loss: 49704.1875\n",
      "Actor Loss: 53821.859375\n",
      "Actor Loss: 53754.61328125\n",
      "Actor Loss: 54604.98828125\n",
      "Actor Loss: 52643.71484375\n",
      "Actor Loss: 57347.82421875\n",
      "Actor Loss: 53576.3125\n",
      "Actor Loss: 36548.74609375\n",
      "Actor Loss: 53324.34765625\n",
      "Actor Loss: 42024.2421875\n",
      "Actor Loss: 32057.37890625\n",
      "Actor Loss: 43813.59375\n",
      "Actor Loss: 53752.0625\n",
      "Epoch 25/300: Mean rewards: 18.18, Std: 7.3\n",
      "Actor Loss: 58875.5859375\n",
      "Actor Loss: 47943.61328125\n",
      "Actor Loss: 186709.75\n",
      "Actor Loss: 65266.88671875\n",
      "Actor Loss: 37143.0078125\n",
      "Actor Loss: 36675.4453125\n",
      "Actor Loss: 37494.73828125\n",
      "Actor Loss: 101718.484375\n",
      "Actor Loss: 70157.1328125\n",
      "Actor Loss: 37326.265625\n",
      "Actor Loss: 35332.36328125\n",
      "Actor Loss: 85472.234375\n",
      "Actor Loss: 51003.5\n",
      "Actor Loss: 29699.3046875\n",
      "Actor Loss: 33321.828125\n",
      "Actor Loss: 32632.45703125\n",
      "Actor Loss: 88198.390625\n",
      "Actor Loss: 29154.677734375\n",
      "Actor Loss: 38223.859375\n",
      "Actor Loss: 28668.19140625\n",
      "Actor Loss: 28480.900390625\n",
      "Actor Loss: 41535.09765625\n",
      "Actor Loss: 33543.8046875\n",
      "Actor Loss: 53096.9765625\n",
      "Actor Loss: 30423.98046875\n",
      "Epoch 50/300: Mean rewards: 20.26, Std: 10.03\n",
      "Actor Loss: 41540.26953125\n",
      "Actor Loss: 35387.37109375\n",
      "Actor Loss: 43727.30078125\n",
      "Actor Loss: 65196.2265625\n",
      "Actor Loss: 35198.07421875\n",
      "Actor Loss: 48900.21875\n",
      "Actor Loss: 48463.08203125\n",
      "Actor Loss: 30170.47265625\n",
      "Actor Loss: 107516.9140625\n",
      "Actor Loss: 38422.85546875\n",
      "Actor Loss: 51601.625\n",
      "Actor Loss: 48320.73828125\n",
      "Actor Loss: 100765.96875\n",
      "Actor Loss: 42997.0859375\n",
      "Actor Loss: 48928.453125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-745-bd6788c49092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-743-0de4dbacbc28>\u001b[0m in \u001b[0;36mtraining_batch\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m  \u001b[0;31m# just observed s_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# act on just observed, action a_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-cf9e1db7713d>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-cf9e1db7713d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(config_a2c)\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 256\n",
    "rewards = agent.training_batch(EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzuNtPqTju64"
   },
   "source": [
    "Evaluate the agent over multiple episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bw279M4Jj1y-"
   },
   "outputs": [],
   "source": [
    "agent.evaluate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MVARL19_part2.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
